"""
åˆ†è¯å™¨è®­ç»ƒè„šæœ¬ - ä½¿ç”¨HuggingFace Tokenizersåº“è®­ç»ƒBPEåˆ†è¯å™¨

åŠŸèƒ½è¯´æ˜ï¼š
æœ¬è„šæœ¬è®­ç»ƒä¸€ä¸ªByte Pair Encoding (BPE) åˆ†è¯å™¨ï¼Œç±»ä¼¼äºGPT-4çš„åˆ†è¯å™¨é£æ ¼ã€‚
åˆ†è¯å™¨æ˜¯è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºtokenåºåˆ—ã€‚

BPEç®—æ³•åŸç†ï¼š
1. ä»å­—ç¬¦çº§åˆ«å¼€å§‹
2. è¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
3. é€æ­¥æ„å»ºè¯æ±‡è¡¨ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å°

è®­ç»ƒè¿‡ç¨‹ï¼š
1. ä»FinewebEduæ•°æ®é›†åŠ è½½è®­ç»ƒæ–‡æœ¬
2. é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„æœ€å¤§é•¿åº¦ï¼ˆé¿å…è¶…é•¿æ–‡æ¡£ï¼‰
3. é™åˆ¶æ€»è®­ç»ƒå­—ç¬¦æ•°ï¼ˆæ§åˆ¶è®­ç»ƒæ—¶é—´ï¼‰
4. ä½¿ç”¨BPEç®—æ³•è®­ç»ƒåˆ†è¯å™¨
5. ä¿å­˜åˆ†è¯å™¨æ¨¡å‹
6. è®¡ç®—å¹¶ç¼“å­˜tokenå­—èŠ‚æ˜ å°„ï¼ˆç”¨äºBPBè¯„ä¼°ï¼‰

è¿è¡Œæ–¹å¼ï¼š

1. é»˜è®¤å‚æ•°ï¼ˆ10Bå­—ç¬¦ï¼Œè¯æ±‡è¡¨å¤§å°65536ï¼‰ï¼š
   python -m scripts.tok_train
   
2. è‡ªå®šä¹‰å‚æ•°ï¼š
   python -m scripts.tok_train --max_chars 5000000000 --vocab_size 32768
   è¯´æ˜ï¼šä½¿ç”¨5Bå­—ç¬¦è®­ç»ƒï¼Œè¯æ±‡è¡¨å¤§å°32768

3. å¿«é€Ÿæµ‹è¯•ï¼ˆå°è¯æ±‡è¡¨ï¼‰ï¼š
   python -m scripts.tok_train --max_chars 100000000 --vocab_size 16384
   è¯´æ˜ï¼šä½¿ç”¨100Må­—ç¬¦å¿«é€Ÿè®­ç»ƒä¸€ä¸ªå°è¯æ±‡è¡¨

æŠ€æœ¯ç‰¹æ€§ï¼š
- Rustå®ç°çš„BPEï¼šä½¿ç”¨rustbpeåº“ï¼Œæ€§èƒ½æé«˜
- æ–‡æ¡£é•¿åº¦é™åˆ¶ï¼šé¿å…è¶…é•¿æ–‡æ¡£å½±å“è®­ç»ƒ
- Tokenå­—èŠ‚æ˜ å°„ï¼šç”¨äºè®¡ç®—bits per byte (BPB)æŒ‡æ ‡
- ç‰¹æ®Štokenå¤„ç†ï¼šæ­£ç¡®å¤„ç†<|bos|>ã€<|user_start|>ç­‰ç‰¹æ®Štoken
- UTF-8å…¼å®¹ï¼šæ­£ç¡®å¤„ç†å¤šå­—èŠ‚å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ã€emojiï¼‰
"""
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import time  # æ—¶é—´æµ‹é‡
import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import torch  # PyTorchï¼ˆç”¨äºä¿å­˜tokenå­—èŠ‚æ˜ å°„ï¼‰
from nanochat.tokenizer import RustBPETokenizer  # Rustå®ç°çš„BPEåˆ†è¯å™¨
from nanochat.common import get_base_dir  # è·å–åŸºç¡€ç›®å½•
from nanochat.dataset import parquets_iter_batched  # Parquetæ•°æ®é›†è¿­ä»£å™¨

# =============================================================================
# è§£æå‘½ä»¤è¡Œå‚æ•°
# =============================================================================

parser = argparse.ArgumentParser(description='è®­ç»ƒBPEåˆ†è¯å™¨')
parser.add_argument('--max_chars', type=int, default=10_000_000_000, help='æœ€å¤šè®­ç»ƒçš„å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š10Bï¼‰')
parser.add_argument('--doc_cap', type=int, default=10_000, help='æ¯ä¸ªæ–‡æ¡£çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š10,000ï¼‰')
parser.add_argument('--vocab_size', type=int, default=65536, help='è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤ï¼š65536 = 2^16ï¼‰')
args = parser.parse_args()
print(f"æœ€å¤§å­—ç¬¦æ•°: {args.max_chars:,}")
print(f"æ–‡æ¡£å­—ç¬¦ä¸Šé™: {args.doc_cap:,}")
print(f"è¯æ±‡è¡¨å¤§å°: {args.vocab_size:,}")

# =============================================================================
# æ–‡æœ¬è¿­ä»£å™¨
# =============================================================================

def text_iterator():
    """
    è®­ç»ƒæ–‡æœ¬è¿­ä»£å™¨
    
    å¤„ç†æ­¥éª¤ï¼š
    1) å°†æ‰¹æ¬¡å±•å¹³ä¸ºå•ä¸ªè¿­ä»£å™¨
    2) è£å‰ªæ¯ä¸ªæ–‡æ¡£åˆ°args.doc_capä¸ªå­—ç¬¦ï¼ˆé¿å…è¶…é•¿æ–‡æ¡£ï¼‰
    3) å½“è¾¾åˆ°args.max_charsä¸ªå­—ç¬¦æ—¶åœæ­¢
    
    ä¸ºä»€ä¹ˆè¦é™åˆ¶æ–‡æ¡£é•¿åº¦ï¼Ÿ
    - è¶…é•¿æ–‡æ¡£ä¼šå¯¼è‡´BPEè®­ç»ƒæ•ˆç‡ä½ä¸‹
    - å¯¹äºå¤§å¤šæ•°tokenåˆå¹¶ï¼Œæ–‡æ¡£å‰10Kå­—ç¬¦å·²ç»è¶³å¤Ÿä»£è¡¨æ€§
    
    ä¸ºä»€ä¹ˆè¦é™åˆ¶æ€»å­—ç¬¦æ•°ï¼Ÿ
    - æ§åˆ¶è®­ç»ƒæ—¶é—´
    - é€šå¸¸10Bå­—ç¬¦å·²ç»è¶³å¤Ÿè®­ç»ƒä¸€ä¸ªé«˜è´¨é‡çš„åˆ†è¯å™¨
    """
    nchars = 0
    for batch in parquets_iter_batched(split="train"):
        for doc in batch:
            doc_text = doc
            # è£å‰ªæ–‡æ¡£åˆ°æœ€å¤§é•¿åº¦
            if len(doc_text) > args.doc_cap:
                doc_text = doc_text[:args.doc_cap]
            nchars += len(doc_text)
            yield doc_text
            # è¾¾åˆ°æœ€å¤§å­—ç¬¦æ•°ååœæ­¢
            if nchars > args.max_chars:
                return

text_iter = text_iterator()

# =============================================================================
# è®­ç»ƒåˆ†è¯å™¨
# =============================================================================
t0 = time.time()
tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)
t1 = time.time()
train_time = t1 - t0
print(f"è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")

# =============================================================================
# ä¿å­˜åˆ†è¯å™¨åˆ°ç£ç›˜
# =============================================================================
base_dir = get_base_dir()
tokenizer_dir = os.path.join(base_dir, "tokenizer")
tokenizer.save(tokenizer_dir)

# =============================================================================
# å¿«é€Ÿå†…è”å¥å…¨æ€§æ£€æŸ¥
# =============================================================================
# æµ‹è¯•å„ç§ç±»å‹çš„æ–‡æœ¬ï¼šæ™®é€šæ–‡æœ¬ã€æ•°å­—ã€ç¼©å†™ã€ç‰¹æ®Šå­—ç¬¦ã€Unicode
test_text = """Hello world! This is a test.
Numbers: 123, 4567, 89
Contractions: I'm, you're, it's
Special chars: @#$%^&*()
Unicode: ä½ å¥½ä¸–ç•Œ ğŸŒ"""
encoded = tokenizer.encode(test_text)
decoded = tokenizer.decode(encoded)
assert decoded == test_text, "åˆ†è¯å™¨ç¼–ç è§£ç æµ‹è¯•å¤±è´¥"

# =============================================================================
# è®¡ç®—å¹¶ç¼“å­˜Tokenå­—èŠ‚æ˜ å°„
# =============================================================================
# ä¸ºä»€ä¹ˆéœ€è¦tokenå­—èŠ‚æ˜ å°„ï¼Ÿ
# ä¸ºäº†é«˜æ•ˆè¯„ä¼°bits per byte (BPB)ã€‚ä¸å…¸å‹çš„å¹³å‡lossä¸åŒï¼Œ
# è¿™å…è®¸æˆ‘ä»¬æŠ¥å‘Šä¸€ä¸ªä¸åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°æ— å…³çš„æŸå¤±ã€‚
# éªŒè¯é›†ä¸Šçš„bits per byteæ˜¯æˆ‘ä»¬å…³å¿ƒçš„ä¸»è¦æŒ‡æ ‡ä¹‹ä¸€ã€‚

vocab_size = tokenizer.get_vocab_size()
special_set = set(tokenizer.get_special_tokens())
token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]
token_bytes = []

for token_id in range(vocab_size):
    token_str = token_strings[token_id]  # æ­¤tokençš„Pythonå­—ç¬¦ä¸²è¡¨ç¤º
    if token_str in special_set:
        token_bytes.append(0)  # ç‰¹æ®Šå­—ç¬¦ä¸è®¡å…¥å­—èŠ‚æ•°
    else:
        id_bytes = len(token_str.encode("utf-8"))  # ç»„æˆæ­¤tokençš„å­—èŠ‚æ•°
        token_bytes.append(id_bytes)

# ä¿å­˜ä¸ºPyTorch tensor
token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')
token_bytes_path = os.path.join(tokenizer_dir, "token_bytes.pt")
with open(token_bytes_path, "wb") as f:
    torch.save(token_bytes, f)
print(f"å·²ä¿å­˜tokenå­—èŠ‚æ˜ å°„åˆ° {token_bytes_path}")

# =============================================================================
# è®°å½•åˆ°å®éªŒæŠ¥å‘Š
# =============================================================================
from nanochat.report import get_report
token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)
get_report().log(section="Tokenizer training", data=[
    vars(args),  # å‘½ä»¤è¡Œå‚æ•°
    {"train_time": train_time},  # è®­ç»ƒæ—¶é—´
    {"num_special_tokens": len(special_set)},  # ç‰¹æ®Štokenæ•°é‡
    {
        "token_bytes_min": int(token_bytes_nonzero.min().item()),
        "token_bytes_max": int(token_bytes_nonzero.max().item()),
        "token_bytes_mean": token_bytes_nonzero.mean().item(),
        "token_bytes_std": token_bytes_nonzero.std().item(),
    }
])
