"""
BPEåˆ†è¯å™¨å®ç°å¯¹æ¯”æµ‹è¯•

è¿™ä¸ªæµ‹è¯•æ–‡ä»¶å¯¹æ¯”äº†å¤šç§BPEï¼ˆByte Pair Encodingï¼‰åˆ†è¯å™¨çš„è®­ç»ƒå®ç°ï¼Œ
ç¡®ä¿å®ƒä»¬äº§ç”Ÿç›¸åŒçš„è¯æ±‡è¡¨ã€åˆå¹¶è§„åˆ™å’Œç¼–ç ç»“æœã€‚

æµ‹è¯•å¯¹è±¡ï¼š
    1. Pythonå‚è€ƒå®ç°ï¼ˆéå¸¸æ…¢ï¼Œç”¨äºéªŒè¯æ­£ç¡®æ€§ï¼‰
    2. ä¼˜åŒ–çš„Pythonå®ç°ï¼ˆä½¿ç”¨åŸåœ°ä¿®æ”¹å’Œå¢é‡æ›´æ–°ï¼‰
    3. HuggingFace tokenizersåº“çš„è®­ç»ƒå®ç°
    4. æˆ‘ä»¬çš„è‡ªå®šä¹‰RustBPEå®ç°ï¼ˆæœ€å¿«ï¼‰

æµ‹è¯•ç›®çš„ï¼š
    - éªŒè¯æ‰€æœ‰å®ç°è®¡ç®—å‡ºç›¸åŒçš„åˆå¹¶åºåˆ—
    - éªŒè¯äº§ç”Ÿç›¸åŒçš„è¯æ±‡è¡¨
    - éªŒè¯å¯¹ç›¸åŒæ–‡æœ¬äº§ç”Ÿç›¸åŒçš„tokenizationç»“æœ
    - éªŒè¯å¯ä»¥å¯¼å‡ºåˆ°tiktokenå¹¶ä¿æŒä¸€è‡´æ€§

è¿è¡Œæ–¹å¼ï¼š
    python -m pytest tests/test_rustbpe.py -v -s
    
å‚æ•°è¯´æ˜ï¼š
    -v: è¯¦ç»†æ¨¡å¼ï¼ˆverboseï¼‰ï¼Œæ˜¾ç¤ºæ¯ä¸ªæµ‹è¯•çš„è¯¦ç»†ä¿¡æ¯
    -s: æ˜¾ç¤ºprintè¾“å‡ºï¼ˆshow printsï¼‰

æŠ€æœ¯èƒŒæ™¯ï¼š
    BPEæ˜¯ä¸€ç§æ•°æ®å‹ç¼©æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºNLPé¢†åŸŸçš„åˆ†è¯ã€‚
    å®ƒé€šè¿‡è¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—èŠ‚å¯¹æ¥æ„å»ºè¯æ±‡è¡¨ã€‚
    æˆ‘ä»¬ä½¿ç”¨tiktokenè¿›è¡Œæ¨ç†ï¼Œå› ä¸ºå®ƒåœ¨Pythonä¸­æä¾›äº†æœ€é«˜æ•ˆçš„ç¼–ç æ€§èƒ½ã€‚
"""

# regexåº“ï¼šæ”¯æŒUnicodeå±æ€§çš„é«˜çº§æ­£åˆ™è¡¨è¾¾å¼
import regex as re
# Counterï¼šè®¡æ•°å™¨ï¼Œdefaultdictï¼šé»˜è®¤å­—å…¸
from collections import Counter, defaultdict
# timeï¼šæ—¶é—´æµ‹é‡
import time
# rustbpeï¼šæˆ‘ä»¬çš„Rustå®ç°çš„BPEåˆ†è¯å™¨
import rustbpe
# tiktokenï¼šOpenAIçš„å¿«é€ŸBPEå®ç°
import tiktoken
# pytestï¼šPythonæµ‹è¯•æ¡†æ¶
import pytest

# GPT-4çš„åˆ†è¯æ¨¡å¼ï¼šå¤„ç†ç¼©å†™ã€Unicodeå­—ç¬¦ã€æ•°å­—ã€æ ‡ç‚¹å’Œç©ºç™½
# è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å®šä¹‰äº†å¦‚ä½•å°†æ–‡æœ¬åˆ‡åˆ†æˆé¢„åˆ†è¯å—
GPT4_SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

# =============================================================================
# Pythonå‚è€ƒå®ç°åˆ†è¯å™¨
# =============================================================================
# è¿™ä¸ªå®ç°ä¸»è¦æ¥è‡ªminbpeé¡¹ç›®ï¼Œç»è¿‡äº†ä¸€äº›ç²¾ç®€
# è™½ç„¶é€Ÿåº¦å¾ˆæ…¢ï¼Œä½†é€»è¾‘æ¸…æ™°ï¼Œç”¨äºéªŒè¯å…¶ä»–å®ç°çš„æ­£ç¡®æ€§

def get_stats(ids, counts=None):
    """
    ç»Ÿè®¡è¿ç»­å­—èŠ‚å¯¹çš„å‡ºç°æ¬¡æ•°
    
    å‚æ•°ï¼š
        ids: æ•´æ•°åˆ—è¡¨ï¼ˆè¡¨ç¤ºå­—èŠ‚åºåˆ—ï¼‰
        counts: å¯é€‰çš„ç°æœ‰è®¡æ•°å­—å…¸ï¼ˆç”¨äºç´¯åŠ ï¼‰
    
    è¿”å›ï¼š
        å­—å…¸ï¼Œé”®ä¸º(int, int)çš„å­—èŠ‚å¯¹ï¼Œå€¼ä¸ºå‡ºç°æ¬¡æ•°
    
    ç¤ºä¾‹ï¼š
        [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    """
    counts = {} if counts is None else counts
    for pair in zip(ids, ids[1:]):  # è¿­ä»£ç›¸é‚»å…ƒç´ 
        counts[pair] = counts.get(pair, 0) + 1
    return counts

def merge(ids, pair, idx):
    """
    åœ¨æ•´æ•°åˆ—è¡¨ä¸­å°†æ‰€æœ‰è¿ç»­å‡ºç°çš„å­—èŠ‚å¯¹æ›¿æ¢ä¸ºæ–°token
    
    å‚æ•°ï¼š
        ids: æ•´æ•°åˆ—è¡¨
        pair: è¦åˆå¹¶çš„å­—èŠ‚å¯¹ (a, b)
        idx: æ–°tokençš„ID
    
    è¿”å›ï¼š
        åˆå¹¶åçš„æ–°æ•´æ•°åˆ—è¡¨
    
    ç¤ºä¾‹ï¼š
        ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    newids = []
    i = 0
    while i < len(ids):
        # å¦‚æœä¸åœ¨æœ€åä¸€ä¸ªä½ç½®ï¼Œä¸”å½“å‰å¯¹åŒ¹é…ï¼Œåˆ™æ›¿æ¢
        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:
            newids.append(idx)
            i += 2
        else:
            newids.append(ids[i])
            i += 1
    return newids

class RegexTokenizer:
    """
    åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„BPEåˆ†è¯å™¨ï¼ˆPythonå‚è€ƒå®ç°ï¼‰
    
    è¿™æ˜¯ä¸€ä¸ªå®Œæ•´ä½†è¾ƒæ…¢çš„å®ç°ï¼Œç”¨äºéªŒè¯å…¶ä»–ä¼˜åŒ–å®ç°çš„æ­£ç¡®æ€§ã€‚
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬é¢„åˆ†å‰²æˆå—ï¼Œç„¶ååœ¨æ¯ä¸ªå—å†…è¿›è¡ŒBPEè®­ç»ƒã€‚
    """

    def __init__(self, pattern=None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        å‚æ•°ï¼š
            pattern: å¯é€‰ï¼Œé¢„åˆ†å‰²çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨GPT-4æ¨¡å¼ï¼‰
        
        å±æ€§ï¼š
            merges: åˆå¹¶è§„åˆ™å­—å…¸ {(int, int): int}
            special_tokens: ç‰¹æ®Štokenå­—å…¸ {str: int}
                ä¾‹å¦‚: {'<|endoftext|>': 100257}
            vocab: è¯æ±‡è¡¨ {int: bytes}
        """
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.merges = {}  # (int, int) -> int
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {}
        self.inverse_special_tokens = {}
        self.vocab = self._build_vocab()

    def _build_vocab(self):
        """
        æ ¹æ®åˆå¹¶è§„åˆ™æ„å»ºè¯æ±‡è¡¨
        
        è¯æ±‡è¡¨æ˜¯ä»åˆå¹¶è§„åˆ™ç¡®å®šæ€§åœ°æ¨å¯¼å‡ºæ¥çš„ï¼š
        - å‰256ä¸ªIDå¯¹åº”å•ä¸ªå­—èŠ‚
        - åç»­IDé€šè¿‡åˆå¹¶è§„åˆ™é€’å½’æ„å»º
        - ç‰¹æ®Štokené™„åŠ åˆ°è¯æ±‡è¡¨æœ«å°¾
        
        è¿”å›ï¼š
            è¯æ±‡è¡¨å­—å…¸ {token_id: bytes}
        """
        # åŸºç¡€è¯æ±‡ï¼š256ä¸ªå•å­—èŠ‚
        vocab = {idx: bytes([idx]) for idx in range(256)}
        # æ ¹æ®åˆå¹¶è§„åˆ™æ·»åŠ å¤šå­—èŠ‚token
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        # æ·»åŠ ç‰¹æ®Štoken
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def train(self, text, vocab_size, verbose=False):
        """
        è®­ç»ƒBPEåˆ†è¯å™¨
        
        å‚æ•°ï¼š
            text: è®­ç»ƒæ–‡æœ¬
            vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°ï¼ˆå¿…é¡» >= 256ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†è®­ç»ƒä¿¡æ¯
        
        è¿”å›ï¼š
            ambiguous: å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦å­˜åœ¨æ­§ä¹‰åˆå¹¶ï¼ˆå³å¤šä¸ªå­—èŠ‚å¯¹æœ‰ç›¸åŒçš„æœ€å¤§è®¡æ•°ï¼‰
        
        ç®—æ³•æµç¨‹ï¼š
            1. å°†æ–‡æœ¬æŒ‰æ­£åˆ™è¡¨è¾¾å¼åˆ‡åˆ†æˆå—
            2. å°†æ¯ä¸ªå—ç¼–ç ä¸ºå­—èŠ‚åºåˆ—
            3. è¿­ä»£num_mergesæ¬¡ï¼š
                - ç»Ÿè®¡æ‰€æœ‰è¿ç»­å­—èŠ‚å¯¹çš„å‡ºç°æ¬¡æ•°
                - é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„å­—èŠ‚å¯¹
                - å°†è¯¥å­—èŠ‚å¯¹åˆå¹¶ä¸ºæ–°token
                - æ›´æ–°è¯æ±‡è¡¨
        """
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦å‡ºç°æ­§ä¹‰åˆå¹¶ï¼ˆå¤šä¸ªå­—èŠ‚å¯¹æœ‰ç›¸åŒçš„æœ€å¤§è®¡æ•°ï¼‰
        ambiguous = False

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬åˆ‡åˆ†æˆå—
        text_chunks = re.findall(self.compiled_pattern, text)

        # è¾“å…¥æ–‡æœ¬é¢„å¤„ç†ï¼šå°†æ¯ä¸ªæ–‡æœ¬å—è½¬æ¢ä¸ºå­—èŠ‚åˆ—è¡¨
        ids = [list(ch.encode("utf-8")) for ch in text_chunks]

        # è¿­ä»£åˆå¹¶æœ€å¸¸è§çš„å­—èŠ‚å¯¹ä»¥åˆ›å»ºæ–°token
        merges = {}  # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)}  # idx -> bytes
        
        for i in range(num_merges):
            # ç»Ÿè®¡æ‰€æœ‰è¿ç»­å­—èŠ‚å¯¹çš„å‡ºç°æ¬¡æ•°
            stats = {}
            for chunk_ids in ids:
                # ä¼ å…¥statsä¼šåŸåœ°æ›´æ–°ï¼Œç´¯åŠ è®¡æ•°
                get_stats(chunk_ids, stats)
            
            # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„å­—èŠ‚å¯¹
            pair = max(stats, key=stats.get)
            
            # æ£€æŸ¥åˆå¹¶æ˜¯å¦å­˜åœ¨æ­§ä¹‰ï¼ˆå³æœ€å¤§å€¼ä¸å”¯ä¸€ï¼‰
            pair_count = stats[pair]
            pairs_with_max_count = [pair for pair, count in stats.items() if count == pair_count]
            if len(pairs_with_max_count) > 1:
                # å­˜åœ¨å¤šä¸ªç›¸åŒè®¡æ•°çš„å­—èŠ‚å¯¹ï¼Œåˆå¹¶é¡ºåºå¯èƒ½ä¸ç¡®å®š
                ambiguous = True
            
            # åˆ›å»ºæ–°tokenï¼šåˆ†é…ä¸‹ä¸€ä¸ªå¯ç”¨ID
            idx = 256 + i
            
            # åœ¨æ‰€æœ‰å‡ºç°ä½ç½®æ›¿æ¢è¯¥å­—èŠ‚å¯¹ä¸ºæ–°token
            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]
            
            # ä¿å­˜åˆå¹¶è§„åˆ™
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            
            # è¯¦ç»†è¾“å‡º
            if verbose:
                print(f"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences")

        # ä¿å­˜ç±»å˜é‡
        self.merges = merges  # ç”¨äºencode()
        self.vocab = vocab    # ç”¨äºdecode()
        return ambiguous

    def _encode_chunk(self, text_bytes):
        """
        å¯¹å•ä¸ªæ–‡æœ¬å—è¿›è¡Œç¼–ç 
        
        å‚æ•°ï¼š
            text_bytes: å­—èŠ‚åºåˆ—
        
        è¿”å›ï¼š
            token IDåˆ—è¡¨
        
        ç®—æ³•ï¼š
            1. å°†å­—èŠ‚è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨ï¼ˆ0-255ï¼‰
            2. å¾ªç¯æŸ¥æ‰¾å¯åˆå¹¶çš„å­—èŠ‚å¯¹ï¼ˆæŒ‰åˆå¹¶é¡ºåºï¼‰
            3. åº”ç”¨åˆå¹¶è§„åˆ™ç›´åˆ°æ— æ³•ç»§ç»­åˆå¹¶
        """
        # é¦–å…ˆå°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸º0-255èŒƒå›´çš„æ•´æ•°
        ids = list(text_bytes)
        while len(ids) >= 2:
            # æ‰¾åˆ°å…·æœ‰æœ€ä½åˆå¹¶ç´¢å¼•çš„å­—èŠ‚å¯¹ï¼ˆå³æœ€æ—©çš„åˆå¹¶è§„åˆ™ï¼‰
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # å·§å¦™ä¹‹å¤„ï¼šå¦‚æœæ²¡æœ‰æ›´å¤šçš„åˆå¹¶è§„åˆ™å¯ç”¨ï¼Œkeyå‡½æ•°ä¼šå¯¹æ¯ä¸ªå­—èŠ‚å¯¹è¿”å›infï¼Œ
            # minä¼šä»»æ„è¿”å›åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå­—èŠ‚å¯¹
            # æˆ‘ä»¬å¯ä»¥é€šè¿‡æˆå‘˜æ£€æŸ¥æ¥æ£€æµ‹è¿™ç§ç»ˆæ­¢æƒ…å†µ
            if pair not in self.merges:
                break  # æ²¡æœ‰æ›´å¤šå¯ä»¥åˆå¹¶çš„äº†
            # å¦åˆ™åˆå¹¶æœ€ä½³å­—èŠ‚å¯¹ï¼ˆæœ€ä½åˆå¹¶ç´¢å¼•ï¼‰
            idx = self.merges[pair]
            ids = merge(ids, pair, idx)
        return ids

    def encode_ordinary(self, text):
        """
        ç¼–ç æ–‡æœ¬ï¼ˆå¿½ç•¥ç‰¹æ®Štokenï¼‰
        
        å‚æ•°ï¼š
            text: è¾“å…¥æ–‡æœ¬å­—ç¬¦ä¸²
        
        è¿”å›ï¼š
            token IDåˆ—è¡¨
        
        è¿‡ç¨‹ï¼š
            1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬åˆ†å‰²æˆå—
            2. åˆ†åˆ«ç¼–ç æ¯ä¸ªå—
            3. è¿æ¥æ‰€æœ‰å—çš„ç»“æœ
        """
        # æŒ‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼å°†æ–‡æœ¬åˆ‡åˆ†æˆå—
        text_chunks = re.findall(self.compiled_pattern, text)
        # åˆ†åˆ«ç¼–ç æ‰€æœ‰æ–‡æœ¬å—ï¼Œç„¶åè¿æ¥ç»“æœ
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8")  # åŸå§‹å­—èŠ‚
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

# =============================================================================
# ä¼˜åŒ–çš„Pythonåˆ†è¯å™¨
# =============================================================================
# è¿™æ˜¯å‚è€ƒå®ç°çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨åŸåœ°ä¿®æ”¹å’Œå¢é‡æ›´æ–°æ¥æé«˜æ€§èƒ½

def fast_merge_inplace(ids, pair, idx):
    """
    åŸåœ°åˆå¹¶ï¼šåœ¨æ•´æ•°åˆ—è¡¨ä¸­åŸåœ°æ›¿æ¢æ‰€æœ‰è¿ç»­å‡ºç°çš„å­—èŠ‚å¯¹
    
    å‚æ•°ï¼š
        ids: æ•´æ•°åˆ—è¡¨ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
        pair: è¦åˆå¹¶çš„å­—èŠ‚å¯¹ (a, b)
        idx: æ–°tokençš„ID
    
    è¿”å›ï¼š
        ä¿®æ”¹åçš„idsï¼ˆä¸ºäº†é“¾å¼è°ƒç”¨ï¼‰
    
    ç¤ºä¾‹ï¼š
        ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    
    ä¼˜åŒ–ç‚¹ï¼š
        ä½¿ç”¨.pop()åŸåœ°åˆ é™¤ï¼Œé¿å…åˆ›å»ºæ–°åˆ—è¡¨ï¼Œå‡å°‘å†…å­˜åˆ†é…
    """
    # æ‰¾åˆ°æ‰€æœ‰å‡ºç°è¯¥å­—èŠ‚å¯¹çš„ä½ç½®
    i = 0
    while i < len(ids) - 1:
        if ids[i] == pair[0] and ids[i+1] == pair[1]:
            ids[i] = idx
            ids.pop(i+1)  # åŸåœ°åˆ é™¤
        else:
            i += 1
    return ids


class FastRegexTokenizer:
    """
    ä¼˜åŒ–çš„åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„BPEåˆ†è¯å™¨
    
    ç›¸æ¯”å‚è€ƒå®ç°ï¼Œå¼•å…¥äº†å¤šé¡¹ä¼˜åŒ–ï¼š
    - å†…è”å‡½æ•°ä»¥å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
    - ä½¿ç”¨.pop()åŸåœ°ä¿®æ”¹åˆ—è¡¨è€Œéåˆ›å»ºæ–°åˆ—è¡¨
    - åˆå¹¶ç›¸åŒçš„æ–‡æœ¬å—ä¸ºå”¯ä¸€å—
    - å¢é‡æ›´æ–°è®¡æ•°ï¼ˆä»…æ›´æ–°å—å½±å“çš„å—ï¼‰
    - ä½ç½®è¿½è¸ªä»¥åŠ é€Ÿåˆå¹¶æ“ä½œ
    """

    def __init__(self, pattern=None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        å‚æ•°ï¼š
            pattern: å¯é€‰ï¼Œé¢„åˆ†å‰²çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨GPT-4æ¨¡å¼ï¼‰
        """
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {}
        self.inverse_special_tokens = {}
        self.merges = {}
        self.vocab = self._build_vocab()

    def _build_vocab(self):
        """æ ¹æ®åˆå¹¶è§„åˆ™ç¡®å®šæ€§åœ°æ„å»ºè¯æ±‡è¡¨"""
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def train(self, text, vocab_size, verbose=False):
        """
        è®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        å¼•å…¥çš„ä¼˜åŒ–ï¼š
        - é€šè¿‡å†…è”å‡½æ•°å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
        - ä½¿ç”¨.pop()åŸåœ°ä¿®æ”¹IDåˆ—è¡¨è€Œéåˆ›å»ºæ–°åˆ—è¡¨
        - å°†ç›¸åŒçš„æ–‡æœ¬å—åˆå¹¶ä¸ºå”¯ä¸€å—ï¼ˆå¤§å¹…å‡å°‘å¤„ç†é‡ï¼‰
        - æ›´æ™ºèƒ½åœ°æ›´æ–°è®¡æ•° - åªæ›´æ–°å—å½±å“çš„å—å‘¨å›´
        - ä½¿ç”¨ä½ç½®è¿½è¸ªé›†åˆå¿«é€Ÿå®šä½åŒ…å«ç‰¹å®šå­—èŠ‚å¯¹çš„å—
        
        å‚æ•°ï¼š
            text: è®­ç»ƒæ–‡æœ¬
            vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°ï¼ˆå¿…é¡» >= 256ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†è®­ç»ƒä¿¡æ¯
        """
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬åˆ‡åˆ†æˆå—
        text_chunks = re.findall(self.compiled_pattern, text)

        # è®¸å¤šæ–‡æœ¬å—æ˜¯ç›¸åŒçš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬"æŠ˜å "ä¸ºå”¯ä¸€å—
        # è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ä¼˜åŒ–ï¼Œå¯ä»¥å¤§å¹…å‡å°‘éœ€è¦å¤„ç†çš„æ•°æ®é‡
        counts = Counter(text_chunks)
        unique_chunks = [ch for ch, count in counts.items()]
        chunk_counts = [count for ch, count in counts.items()]

        # è¾“å…¥æ–‡æœ¬é¢„å¤„ç†
        ids = [list(ch.encode("utf-8")) for ch in unique_chunks]
        # è¿­ä»£åˆå¹¶æœ€å¸¸è§çš„å­—èŠ‚å¯¹ä»¥åˆ›å»ºæ–°token
        merges = {}  # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)}  # idx -> bytes

        # åˆå§‹è®¡æ•°ï¼šæ„å»ºç»Ÿè®¡ä¿¡æ¯å’Œä½ç½®è¿½è¸ª
        stats = defaultdict(int)
        positions = defaultdict(set)  # pair -> åŒ…å«æ­¤å­—èŠ‚å¯¹çš„chunkç´¢å¼•é›†åˆ

        # éå†æ‰€æœ‰å”¯ä¸€å—ï¼Œåˆå§‹åŒ–ç»Ÿè®¡å’Œä½ç½®ä¿¡æ¯
        for chunk_idx, (chunk_ids, count) in enumerate(zip(ids, chunk_counts)):
            for pair in zip(chunk_ids, chunk_ids[1:]):
                stats[pair] += count  # åŠ æƒè®¡æ•°ï¼ˆä¹˜ä»¥å—çš„é‡å¤æ¬¡æ•°ï¼‰
                positions[pair].add(chunk_idx)  # è®°å½•åŒ…å«æ­¤å­—èŠ‚å¯¹çš„å—

        # ========== ä¸»åˆå¹¶å¾ªç¯ ==========
        for i in range(num_merges):
            if not stats:
                break

            # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„å­—èŠ‚å¯¹
            pair = max(stats, key=stats.get)
            # åˆ›å»ºæ–°tokenï¼šåˆ†é…ä¸‹ä¸€ä¸ªå¯ç”¨ID
            idx = 256 + i

            # è·å–åŒ…å«æ­¤å­—èŠ‚å¯¹çš„æ‰€æœ‰å—ï¼ˆå…³é”®ä¼˜åŒ–ï¼šåªå¤„ç†å—å½±å“çš„å—ï¼‰
            affected_chunks = positions[pair]

            # è¿½è¸ªè®¡æ•°å˜åŒ–ä»¥è¿›è¡Œå¢é‡æ›´æ–°ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
            count_changes = defaultdict(int)

            # åªåœ¨å—å½±å“çš„å—ä¸­æ›¿æ¢å­—èŠ‚å¯¹çš„æ‰€æœ‰å‡ºç°
            for chunk_idx in affected_chunks:
                chunk_ids = ids[chunk_idx]
                chunk_count = chunk_counts[chunk_idx]  # æ­¤å—çš„é‡å¤æ¬¡æ•°
                ix = 0
                while ix < len(chunk_ids) - 1:
                    if chunk_ids[ix] == pair[0] and chunk_ids[ix+1] == pair[1]:
                        # è¿½è¸ªæ­£åœ¨è¢«ç§»é™¤/æ·»åŠ çš„å­—èŠ‚å¯¹
                        # ç§»é™¤: (prev, A), (A, B), (B, next)
                        # å…¶ä¸­(A, B)æ˜¯è¦åˆå¹¶çš„å­—èŠ‚å¯¹
                        
                        # å¦‚æœä¸åœ¨å¼€å¤´ï¼Œç§»é™¤å·¦ä¾§å­—èŠ‚å¯¹ (prev, A)
                        if ix > 0:
                            old_left = (chunk_ids[ix-1], chunk_ids[ix])
                            count_changes[old_left] -= chunk_count

                        # è¢«åˆå¹¶çš„å­—èŠ‚å¯¹æ¶ˆå¤±
                        count_changes[pair] -= chunk_count

                        # å¦‚æœä¸åœ¨å€’æ•°ç¬¬äºŒä½ï¼Œç§»é™¤å³ä¾§å­—èŠ‚å¯¹ (B, next)
                        if ix + 2 < len(chunk_ids):
                            old_right = (chunk_ids[ix+1], chunk_ids[ix+2])
                            count_changes[old_right] -= chunk_count

                        # åº”ç”¨åˆå¹¶ï¼šå°†(A, B)æ›¿æ¢ä¸ºC
                        chunk_ids[ix] = idx
                        chunk_ids.pop(ix+1)  # åŸåœ°åˆ é™¤

                        # æ·»åŠ : (prev, C), (C, next)
                        # æ–°çš„å­—èŠ‚å¯¹å‡ºç°
                        
                        # å¦‚æœä¸åœ¨å¼€å¤´ï¼Œæ·»åŠ æ–°çš„å·¦ä¾§å­—èŠ‚å¯¹ (prev, C)
                        if ix > 0:
                            new_left = (chunk_ids[ix-1], chunk_ids[ix])
                            count_changes[new_left] += chunk_count

                        # å¦‚æœä¸åœ¨æœ«å°¾ï¼Œæ·»åŠ æ–°çš„å³ä¾§å­—èŠ‚å¯¹ (C, next)
                        if ix + 1 < len(chunk_ids):
                            new_right = (chunk_ids[ix], chunk_ids[ix+1])
                            count_changes[new_right] += chunk_count
                    else:
                        ix += 1

            # åº”ç”¨å¢é‡å˜åŒ–åˆ°ç»Ÿè®¡ä¿¡æ¯å’Œä½ç½®ï¼ˆå…³é”®ä¼˜åŒ–ï¼šé¿å…é‡æ–°è®¡ç®—æ‰€æœ‰å—ï¼‰
            for changed_pair, delta in count_changes.items():
                if changed_pair == pair:
                    # è¢«åˆå¹¶çš„å­—èŠ‚å¯¹åº”è¯¥å®Œå…¨æ¶ˆå¤±
                    continue

                stats[changed_pair] += delta

                # æ›´æ–°å˜åŒ–å­—èŠ‚å¯¹çš„ä½ç½® - åªæ£€æŸ¥å—å½±å“çš„å—
                for chunk_idx in affected_chunks:
                    chunk_ids = ids[chunk_idx]
                    contains_pair = any((chunk_ids[j], chunk_ids[j+1]) == changed_pair
                                      for j in range(len(chunk_ids) - 1))
                    if contains_pair:
                        positions[changed_pair].add(chunk_idx)
                    else:
                        positions[changed_pair].discard(chunk_idx)

            # å®Œå…¨ç§»é™¤è¢«åˆå¹¶çš„å­—èŠ‚å¯¹
            del stats[pair]
            del positions[pair]

            # ä¿å­˜åˆå¹¶è§„åˆ™
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]

        # ä¿å­˜ç±»å˜é‡
        self.merges = merges  # ç”¨äºencode()
        self.vocab = vocab    # ç”¨äºdecode()

    def register_special_tokens(self, special_tokens):
        """
        æ³¨å†Œç‰¹æ®Štoken
        
        å‚æ•°ï¼š
            special_tokens: å­—å…¸ {str: int}
                ä¾‹å¦‚: {"<|endoftext|>": 100257}
        """
        self.special_tokens = special_tokens
        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}

    def decode(self, ids):
        """
        è§£ç token IDåºåˆ—ä¸ºæ–‡æœ¬
        
        å‚æ•°ï¼š
            ids: token IDåˆ—è¡¨
        
        è¿”å›ï¼š
            è§£ç åçš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        part_bytes = []
        for idx in ids:
            if idx in self.vocab:
                part_bytes.append(self.vocab[idx])
            elif idx in self.inverse_special_tokens:
                part_bytes.append(self.inverse_special_tokens[idx].encode("utf-8"))
            else:
                raise ValueError(f"invalid token id: {idx}")
        text_bytes = b"".join(part_bytes)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def _encode_chunk(self, text_bytes):
        """å¯¹å•ä¸ªæ–‡æœ¬å—è¿›è¡Œç¼–ç ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„åŸåœ°åˆå¹¶ï¼‰"""
        ids = list(text_bytes)
        while len(ids) >= 2:
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            if pair not in self.merges:
                break
            idx = self.merges[pair]
            ids = fast_merge_inplace(ids, pair, idx)  # ä½¿ç”¨ä¼˜åŒ–çš„åŸåœ°åˆå¹¶
        return ids

    def encode_ordinary(self, text):
        """ç¼–ç æ–‡æœ¬ï¼ˆå¿½ç•¥ç‰¹æ®Štokenï¼‰"""
        text_chunks = re.findall(self.compiled_pattern, text)
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8")
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

# =============================================================================
# HuggingFace tokenizersåº“å°è£…
# =============================================================================
from tokenizers import Tokenizer as HFTokenizer
from tokenizers import pre_tokenizers, decoders, Regex
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer

class HuggingFaceTokenizer:
    """
    HuggingFace Tokenizerçš„è½»é‡çº§å°è£…
    
    ç”¨äºå¯¹æ¯”æµ‹è¯•ï¼ŒéªŒè¯æˆ‘ä»¬çš„å®ç°ä¸HuggingFaceçš„å®ç°ç»“æœä¸€è‡´ã€‚
    HuggingFace tokenizersæ˜¯ç”¨Rustç¼–å†™çš„ï¼Œæ€§èƒ½å¾ˆå¥½ï¼Œä½†å­—èŠ‚é¡ºåºå¯èƒ½ä¸åŒã€‚
    """

    def __init__(self, tokenizer):
        """
        åˆå§‹åŒ–å°è£…å™¨
        
        å‚æ•°ï¼š
            tokenizer: HuggingFaceçš„Tokenizerå®ä¾‹
        """
        self.tokenizer = tokenizer

    @classmethod
    def train_from_iterator(cls, text_iterator, vocab_size):
        """
        ä»æ–‡æœ¬è¿­ä»£å™¨è®­ç»ƒåˆ†è¯å™¨
        
        å‚æ•°ï¼š
            text_iterator: æ–‡æœ¬è¿­ä»£å™¨
            vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°
        
        è¿”å›ï¼š
            HuggingFaceTokenizerå®ä¾‹
        
        é…ç½®è¯´æ˜ï¼š
            - BPEæ¨¡å‹with byte_fallbackï¼ˆå¿…éœ€ï¼‰
            - æ— Normalizer
            - GPT-4é£æ ¼çš„Pre-tokenizer
            - ByteLevelè§£ç å™¨
            - æ— Post-processor
        """
        # é…ç½®HuggingFace Tokenizer
        tokenizer = HFTokenizer(BPE(
            byte_fallback=True,  # å¿…éœ€ï¼šæ”¯æŒå­—èŠ‚çº§å›é€€
            unk_token=None,
            fuse_unk=False,
        ))
        # Normalizerï¼šæ— ï¼ˆä¸åšæ–‡æœ¬æ ‡å‡†åŒ–ï¼‰
        tokenizer.normalizer = None
        # Pre-tokenizerï¼šGPT-4é£æ ¼
        gpt4_split_regex = Regex(GPT4_SPLIT_PATTERN)  # HuggingFaceè¦æ±‚åŒ…è£…åœ¨Regexä¸­
        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior="isolated", invert=False),
            pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)
        ])
        # Decoderï¼šByteLevelï¼ˆä¸ByteLevel pre-tokenizeré…å¯¹ï¼‰
        tokenizer.decoder = decoders.ByteLevel()
        # Post-processorï¼šæ— 
        tokenizer.post_processor = None
        # Trainerï¼šBPEè®­ç»ƒå™¨
        trainer = BpeTrainer(
            vocab_size=vocab_size,
            show_progress=True,
            min_frequency=0,  # æ— æœ€å°é¢‘ç‡è¦æ±‚
            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
            special_tokens=[],  # æ— ç‰¹æ®Štoken
        )
        # å¯åŠ¨è®­ç»ƒ
        tokenizer.train_from_iterator(text_iterator, trainer)
        return cls(tokenizer)

    def encode_ordinary(self, text):
        """ç¼–ç æ–‡æœ¬ï¼ˆä¸æ·»åŠ ç‰¹æ®Štokenï¼‰"""
        ids = self.tokenizer.encode(text, add_special_tokens=False).ids
        return ids

# =============================================================================
# æµ‹è¯•å‡½æ•°
# =============================================================================

@pytest.fixture(scope="module")
def enwik8_path():
    """
    pytest fixtureï¼šä¸‹è½½å¹¶ç¼“å­˜enwik8æ•°æ®é›†
    
    enwik8æ˜¯ä¸€ä¸ª100MBçš„Wikipediaæ–‡æœ¬æ•°æ®é›†ï¼Œå¸¸ç”¨äºå‹ç¼©å’Œåˆ†è¯åŸºå‡†æµ‹è¯•ã€‚
    """
    import os
    import zipfile
    from nanochat.common import get_base_dir
    base_dir = get_base_dir()
    # ä¸‹è½½å¹¶è§£å‹enwik8åˆ°.cacheç›®å½•
    enwik8_url = "https://mattmahoney.net/dc/enwik8.zip"
    enwik8_local_path = os.path.join(base_dir, "enwik8")
    enwik8_local_path_zip = os.path.join(base_dir, "enwik8.zip")
    if not os.path.exists(enwik8_local_path):
        print(f"Downloading enwik8 to {enwik8_local_path_zip}")
        import requests
        response = requests.get(enwik8_url)
        with open(enwik8_local_path_zip, "wb") as f:
            f.write(response.content)
        with zipfile.ZipFile(enwik8_local_path_zip, "r") as zip_ref:
            zip_ref.extractall(base_dir)
        print(f"Unzipped enwik8 to {enwik8_local_path}")
        os.remove(enwik8_local_path_zip)
        print(f"Removed {enwik8_local_path_zip}")
    else:
        print(f"Using existing enwik8 at {enwik8_local_path}")
    return enwik8_local_path


@pytest.fixture(scope="module")
def enwik8_small(enwik8_path):
    """pytest fixtureï¼šæä¾›100KBçš„enwik8æ•°æ®ç”¨äºå¿«é€Ÿæµ‹è¯•"""
    with open(enwik8_path, "r", encoding="utf-8") as f:
        return f.read(100_000)

@pytest.fixture(scope="module")
def enwik8_large(enwik8_path):
    """pytest fixtureï¼šæä¾›10MBçš„enwik8æ•°æ®ç”¨äºæ€§èƒ½æµ‹è¯•"""
    with open(enwik8_path, "r", encoding="utf-8") as f:
        return f.read(10**7)

def time_function(func, *args, **kwargs):
    """
    æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´
    
    å‚æ•°ï¼š
        func: è¦æµ‹é‡çš„å‡½æ•°
        *args, **kwargs: ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°
    
    è¿”å›ï¼š
        (result, elapsed): å‡½æ•°ç»“æœå’Œè¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    """
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    elapsed = end_time - start_time
    return result, elapsed

def test_correctness(enwik8_small):
    """
    æ­£ç¡®æ€§æµ‹è¯•ï¼šéªŒè¯æ‰€æœ‰åˆ†è¯å™¨å®ç°äº§ç”Ÿç›¸åŒçš„ç»“æœ
    
    æµ‹è¯•æµç¨‹ï¼š
        1. è®­ç»ƒæ…¢é€Ÿå‚è€ƒå®ç°ï¼ˆPythonï¼‰
        2. è®­ç»ƒå¿«é€Ÿå‚è€ƒå®ç°ï¼ˆä¼˜åŒ–çš„Pythonï¼‰
        3. è®­ç»ƒHuggingFaceå®ç°
        4. è®­ç»ƒRustBPEå®ç°
        5. éªŒè¯æ‰€æœ‰å®ç°äº§ç”Ÿç›¸åŒçš„ç¼–ç ç»“æœ
        6. éªŒè¯RustBPEå¯ä»¥å¯¼å‡ºåˆ°tiktokenå¹¶ä¿æŒä¸€è‡´
    
    å‚æ•°ï¼š
        enwik8_small: 100KBçš„enwik8æµ‹è¯•æ•°æ®ï¼ˆæ¥è‡ªfixtureï¼‰
    """
    text = enwik8_small
    encode_text = text
    vocab_size = 256 + 20  # åŸºç¡€256å­—èŠ‚ + 20æ¬¡åˆå¹¶

    # ========== è®­ç»ƒæ…¢é€Ÿå‚è€ƒå®ç° ==========
    print("\nTraining slow reference...")
    slow_reference_tokenizer = RegexTokenizer()
    ambiguous_flag, slow_reference_train_time = time_function(slow_reference_tokenizer.train, text, vocab_size)
    slow_reference_ids, slow_reference_encode_time = time_function(slow_reference_tokenizer.encode_ordinary, encode_text)
    print(f"Slow reference train time: {slow_reference_train_time:.4f}s")
    print(f"Slow reference encode time: {slow_reference_encode_time:.4f}s")
    print(slow_reference_ids[:20])

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ­§ä¹‰åˆå¹¶
    if ambiguous_flag:
        print("â€¼ï¸ WARNING: merge order was detected to be ambiguous given current text and vocab size")
        print("The implementation could be correct but we might see different results below")
    else:
        print("âœ… Merge order is NOT ambiguous")

    # ========== è®­ç»ƒå¿«é€Ÿå‚è€ƒå®ç° ==========
    print("\nTraining fast reference...")
    fast_reference_tokenizer = FastRegexTokenizer()
    _, fast_reference_train_time = time_function(fast_reference_tokenizer.train, text, vocab_size)
    fast_reference_ids, fast_reference_encode_time = time_function(fast_reference_tokenizer.encode_ordinary, encode_text)
    print(f"Fast reference train time: {fast_reference_train_time:.4f}s")
    print(f"Fast reference encode time: {fast_reference_encode_time:.4f}s")
    print(fast_reference_ids[:20])

    # éªŒè¯å¿«é€Ÿç‰ˆæœ¬ä¸æ…¢é€Ÿç‰ˆæœ¬ä¸€è‡´
    assert fast_reference_ids == slow_reference_ids, "Fast reference should match slow reference"
    print("âœ… Fast == Slow")

    # ========== è®­ç»ƒHuggingFaceå®ç° ==========
    print("\nTraining HuggingFace...")
    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)
    hf_ids, hf_encode_time = time_function(hf_tokenizer.encode_ordinary, encode_text)
    print(f"HuggingFace train time: {hf_train_time:.4f}s")
    print(f"HuggingFace encode time: {hf_encode_time:.4f}s")
    print(hf_ids[:20])

    # HuggingFaceä½¿ç”¨ä¸åŒçš„å­—èŠ‚é¡ºåºï¼Œæ‰€ä»¥éœ€è¦è‡ªå®šä¹‰åŒ¹é…é€»è¾‘
    def custom_match(ids1, ids2):
        """
        è‡ªå®šä¹‰åŒ¹é…å‡½æ•°ï¼šè€ƒè™‘HuggingFaceçš„å­—èŠ‚é¡ºåºå¯èƒ½ä¸åŒ
        
        è§„åˆ™ï¼š
        - å•å­—èŠ‚tokenï¼ˆ<256ï¼‰å¯ä»¥æœ‰ä¸åŒçš„æ˜ å°„ï¼ˆå­—èŠ‚é¡ºåºä¸åŒï¼‰
        - åˆå¹¶tokenï¼ˆ>=256ï¼‰å¿…é¡»å®Œå…¨ç›¸åŒ
        """
        perm = {}
        for x, y in zip(ids1, ids2):
            if x < 256:
                if x in perm:
                    if perm[x] != y:
                        return False
                perm[x] = y
            if x >= 256 and x != y:
                return False
        return True

    assert custom_match(hf_ids, fast_reference_ids), "HuggingFace should match fast reference"
    print("âœ… HuggingFace == Fast")

    # ========== è®­ç»ƒæˆ‘ä»¬çš„Rustå®ç° ==========
    print("\nTraining rustbpe...")
    rustbpe_tokenizer = rustbpe.Tokenizer()
    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)
    rustbpe_ids, rustbpe_encode_time = time_function(rustbpe_tokenizer.encode, encode_text)
    print(f"RustBPE train time: {rustbpe_train_time:.4f}s")
    print(f"RustBPE encode time: {rustbpe_encode_time:.4f}s")
    print(rustbpe_ids[:20])

    assert rustbpe_ids == fast_reference_ids, "RustBPE should match fast reference"
    print("âœ… RustBPE == Fast")

    # ========== æµ‹è¯•å¯¼å‡ºåˆ°tiktoken ==========
    # å¯¹äºç”Ÿäº§ç¯å¢ƒæ¨ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨tiktokenä»¥è·å¾—æœ€ä½³æ€§èƒ½
    print("\nTesting tiktoken export...")
    pattern = rustbpe_tokenizer.get_pattern()
    mergeable_ranks_list = rustbpe_tokenizer.get_mergeable_ranks()
    mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}
    enc = tiktoken.Encoding(
        name="rustbpe",
        pat_str=pattern,
        mergeable_ranks=mergeable_ranks,
        special_tokens={},
    )
    tiktoken_ids, tiktoken_encode_time = time_function(enc.encode, encode_text)
    print(f"Tiktoken encode time: {tiktoken_encode_time:.4f}s")
    print(tiktoken_ids[:20])

    assert tiktoken_ids == rustbpe_ids, "Tiktoken should match RustBPE"
    print("âœ… Tiktoken == RustBPE")


@pytest.mark.slow
def test_training_performance(enwik8_large):
    """
    æ€§èƒ½æµ‹è¯•ï¼šä½¿ç”¨å¤§æ•°æ®é›†å¯¹æ¯”è®­ç»ƒé€Ÿåº¦
    
    è¿™ä¸ªæµ‹è¯•ä½¿ç”¨10MBçš„æ•°æ®å’Œ2048çš„è¯æ±‡è¡¨å¤§å°ï¼Œ
    å¯¹æ¯”RustBPEå’ŒHuggingFaceçš„è®­ç»ƒé€Ÿåº¦ã€‚
    
    æ³¨æ„ï¼š
        - æ ‡è®°ä¸º@pytest.mark.slowï¼Œéœ€è¦æ˜¾å¼è¿è¡Œ
        - ä¼˜åŒ–çš„Pythonç‰ˆæœ¬å·²æ³¨é‡Šæ‰ï¼ˆå¤ªæ…¢äº†ï¼‰
    
    å‚æ•°ï¼š
        enwik8_large: 10MBçš„enwik8æµ‹è¯•æ•°æ®ï¼ˆæ¥è‡ªfixtureï¼‰
    """
    text = enwik8_large
    vocab_size = 2048
    print(f"\nText length: {len(text)}")

    # æ³¨é‡Šæ‰Pythonä¼˜åŒ–ç‰ˆæœ¬ï¼Œå› ä¸ºå¤ªæ…¢äº†
    # åœ¨å¤§æ•°æ®é›†ä¸Šï¼ŒRustå’ŒHuggingFaceçš„å®ç°éƒ½æ¯”Pythonå¿«å¾—å¤š
    # print("Training optimized python version...")
    # optimized_python_tokenizer = FastRegexTokenizer()
    # _, optimized_python_train_time = time_function(optimized_python_tokenizer.train, text, vocab_size)
    # print(f"Optimized python train time: {optimized_python_train_time:.4f}s")

    # ========== è®­ç»ƒRustBPE ==========
    print("\nTraining rustbpe...")
    rustbpe_tokenizer = rustbpe.Tokenizer()
    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)
    print(f"RustBPE train time: {rustbpe_train_time:.4f}s")
    assert rustbpe_train_time > 0, "Training should take some time"

    # ========== è®­ç»ƒHuggingFace ==========
    print("\nTraining HuggingFace...")
    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)
    print(f"HuggingFace train time: {hf_train_time:.4f}s")
    assert hf_train_time > 0, "Training should take some time"

    # ========== æ‰“å°æ€§èƒ½å¯¹æ¯” ==========
    print(f"\nğŸ“Š Performance comparison:")
    print(f"   RustBPE: {rustbpe_train_time:.4f}s")
    print(f"   HuggingFace: {hf_train_time:.4f}s")
    print(f"   Speedup: {hf_train_time/rustbpe_train_time:.2f}x")

def test_interface(enwik8_small):
    """
    æ¥å£æµ‹è¯•ï¼šæµ‹è¯•RustBPETokenizerçš„å®Œæ•´æ¥å£
    
    æµ‹è¯•å†…å®¹ï¼š
        1. è®­ç»ƒåˆ†è¯å™¨
        2. ç¼–ç /è§£ç æ–‡æœ¬ï¼ˆåŒ…æ‹¬Unicodeï¼‰
        3. æ‰¹é‡ç¼–ç 
        4. ç‰¹æ®Štokençš„æ·»åŠ ï¼ˆprepend/appendï¼‰
        5. ä¿å­˜å’ŒåŠ è½½åˆ†è¯å™¨
    
    è¿™ä¸ªæµ‹è¯•éªŒè¯nanochat.tokenizer.RustBPETokenizeråŒ…è£…ç±»çš„å®Œæ•´åŠŸèƒ½ã€‚
    
    å‚æ•°ï¼š
        enwik8_small: 100KBçš„enwik8æµ‹è¯•æ•°æ®ï¼ˆæ¥è‡ªfixtureï¼‰
    """
    import tempfile
    from nanochat.tokenizer import RustBPETokenizer

    # ========== æµ‹è¯•1ï¼šè®­ç»ƒåˆ†è¯å™¨ ==========
    vocab_size = 300
    tok = RustBPETokenizer.train_from_iterator([enwik8_small], vocab_size)
    assert tok.get_vocab_size() == vocab_size, f"Expected vocab size {vocab_size}, got {tok.get_vocab_size()}"
    print(f"âœ… Trained tokenizer with vocab size {vocab_size}")

    # ========== æµ‹è¯•2ï¼šç¼–ç /è§£ç ï¼ˆåŒ…æ‹¬emojiï¼‰ ==========
    encode_text = "Hello world! How are you? ğŸ™ƒ"
    ids = tok.encode(encode_text)
    print(f"\nInput text: {encode_text}")
    print(f"IDs: {ids}")
    decoded = tok.decode(ids)
    print(f"Decoded: {decoded}")
    assert decoded == encode_text, f"Decoded text doesn't match: {decoded} != {encode_text}"
    print("âœ… Encode/decode test passed")

    # ========== æµ‹è¯•3ï¼šæ‰¹é‡ç¼–ç  ==========
    ids_new = tok.encode([encode_text, encode_text])
    assert all(x == ids for x in ids_new), "Batch encoding should produce identical results"
    print("âœ… Encode batch OK")

    # ========== æµ‹è¯•4ï¼šç‰¹æ®Štokenæ·»åŠ ï¼ˆprepend/appendï¼‰ ==========
    ids_special = tok.encode(encode_text, prepend="<|bos|>", append="<|bos|>")
    bos_token_id = tok.encode_special("<|bos|>")
    assert ids_special == [bos_token_id] + ids + [bos_token_id], "Special tokens not correctly added"
    print("âœ… append/prepend OK")

    # ========== æµ‹è¯•5ï¼šä¿å­˜å’ŒåŠ è½½ ==========
    with tempfile.TemporaryDirectory() as tmp_dir:
        tok.save(tmp_dir)
        tok_reloaded = RustBPETokenizer.from_directory(tmp_dir)
        ids_reloaded = tok_reloaded.encode(encode_text)
        assert ids_reloaded == ids, "Reloaded tokenizer should produce same results"
        print("âœ… Save/load through temporary directory OK")
