# æ¨¡å‹è§„æ¨¡ä¸ä¸­æ–‡è¯­æ–™é€‚é…æŒ‡å—

> è¯¦ç»†è®¡ç®— d4-d32 å„è§„æ¨¡æ¨¡å‹å‚æ•°ï¼Œå¹¶åˆ†æä¸­æ–‡/ä¸­è‹±æ–‡æ··åˆè¯­æ–™çš„è®­ç»ƒç­–ç•¥

---

## ç›®å½•

- [ä¸€ã€ä¸åŒæ¨¡å‹è§„æ¨¡å‚æ•°è¯¦è§£](#ä¸€ä¸åŒæ¨¡å‹è§„æ¨¡å‚æ•°è¯¦è§£)
- [äºŒã€ä¸­æ–‡è¯­æ–™çš„ç‰¹æ®Šæ€§](#äºŒä¸­æ–‡è¯­æ–™çš„ç‰¹æ®Šæ€§)
- [ä¸‰ã€ä¸­æ–‡/ä¸­è‹±æ–‡è®­ç»ƒé€‚é…ç­–ç•¥](#ä¸‰ä¸­æ–‡ä¸­è‹±æ–‡è®­ç»ƒé€‚é…ç­–ç•¥)
- [å››ã€å®æˆ˜é…ç½®å»ºè®®](#å››å®æˆ˜é…ç½®å»ºè®®)

---

## ä¸€ã€ä¸åŒæ¨¡å‹è§„æ¨¡å‚æ•°è¯¦è§£

### 1.1 å‚æ•°è®¡ç®—å…¬å¼

GPT æ¨¡å‹çš„å‚æ•°é‡ç”±ä»¥ä¸‹å…¬å¼å†³å®šï¼š

```python
# æ ¸å¿ƒå…¬å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
num_params â‰ˆ 12 Ã— n_layer Ã— n_embdÂ² / (n_head Ã— head_dim) + vocab_size Ã— n_embd Ã— 2

# è¯¦ç»†åˆ†è§£ï¼š
# 1. Transformer å±‚å‚æ•°ï¼ˆå å¤§å¤´ï¼‰
#    æ¯å±‚åŒ…å«ï¼š
#    - æ³¨æ„åŠ›: 4 Ã— n_embdÂ² (Q, K, V, O æŠ•å½±)
#    - MLP: 8 Ã— n_embdÂ² (up æŠ•å½± 4Ã—, down æŠ•å½± 4Ã—)
#    - æ€»è®¡: 12 Ã— n_embdÂ² per layer
#    
# 2. åµŒå…¥å±‚å‚æ•°
#    - Token åµŒå…¥: vocab_size Ã— n_embd
#    - LM Head: n_embd Ã— vocab_size (ä¸å…±äº«æƒé‡)
#    
# 3. å½’ä¸€åŒ–å±‚
#    - RMSNorm æ— å‚æ•°ï¼ˆnanochat ç‰¹æ€§ï¼‰

# nanochat çš„è®¾è®¡è§„åˆ™ï¼š
n_layer = depth
n_embd = depth Ã— 64  # aspect ratio = 64
n_head = max(1, (n_embd + 127) // 128)  # head_dim = 128
vocab_size = 65536  # å›ºå®šï¼ˆç”±åˆ†è¯å™¨å†³å®šï¼‰
```

### 1.2 å„è§„æ¨¡æ¨¡å‹å‚æ•°å¯¹ç…§è¡¨

| depth | n_embd | n_head | å‚æ•°é‡ (M) | é¢„è®­ç»ƒ tokens (20Ã—) | é¢„è®­ç»ƒ shards | è®­ç»ƒæ—¶é—´ (8xH100) | æˆæœ¬ ($24/hr) |
|-------|--------|--------|-----------|-------------------|--------------|----------------|---------------|
| **d4** | 256 | 2 | ~37M | 0.74B | 8 | ~15min | ~$6 |
| **d6** | 384 | 3 | ~56M | 1.12B | 12 | ~25min | ~$10 |
| **d8** | 512 | 4 | ~82M | 1.64B | 18 | ~35min | ~$14 |
| **d10** | 640 | 5 | ~112M | 2.24B | 24 | ~50min | ~$20 |
| **d12** | 768 | 6 | ~149M | 2.98B | 32 | ~1.2hr | ~$29 |
| **d16** | 1024 | 8 | ~268M | 5.36B | 58 | ~2hr | ~$48 |
| **d20** | 1280 | 10 | ~561M | 11.2B | 240 | ~4hr | ~$96 |
| **d26** | 1664 | 13 | ~1,200M | 24B | 480 | ~12hr | ~$288 |
| **d32** | 2048 | 16 | ~1,900M | 38B | 720 | ~33hr | ~$792 |

**æ³¨æ„äº‹é¡¹**ï¼š
- å‚æ•°é‡æ˜¯å®é™…è¿è¡Œæ¨¡å‹ç»Ÿè®¡çš„è¿‘ä¼¼å€¼
- è®­ç»ƒæ—¶é—´å‡è®¾ MFU â‰ˆ 40%
- æˆæœ¬åŸºäº Lambda Labs çš„ 8xH100 èŠ‚ç‚¹ï¼ˆ$24/hrï¼‰
- Shards æ•°å‘ä¸Šå–æ•´å¹¶ç•™ ~10% ä½™é‡

### 1.3 è¯¦ç»†è®¡ç®—ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šd4 æ¨¡å‹ï¼ˆæœ€å°å¯è¡Œæ¨¡å‹ï¼‰

```python
# ============ æ¨¡å‹æ¶æ„ ============
depth = 4
n_embd = 4 Ã— 64 = 256
n_head = max(1, (256 + 127) // 128) = max(1, 2) = 2
n_layer = 4
vocab_size = 65536

# ============ å‚æ•°ç»Ÿè®¡ ============
# Transformer å±‚:
#   æ¯å±‚: 12 Ã— 256Â² = 786,432 params
#   4 å±‚: 4 Ã— 786,432 = 3,145,728 params
# 
# åµŒå…¥å±‚:
#   Token emb: 65536 Ã— 256 = 16,777,216 params
#   LM Head: 256 Ã— 65536 = 16,777,216 params
# 
# æ€»è®¡: 3.1M + 16.8M + 16.8M â‰ˆ 37M params

num_params â‰ˆ 37,000,000

# ============ è®­ç»ƒå‚æ•°ï¼ˆChinchilla 20Ã—ï¼‰============
target_tokens = 20 Ã— 37,000,000 = 740,000,000 tokens
num_iterations = 740,000,000 / 524,288 = 1,411 steps

# ============ æ•°æ®éœ€æ±‚ ============
chars_per_token = 4.8  # è‹±æ–‡æ•°æ®
total_chars = 740,000,000 Ã— 4.8 = 3,552,000,000 chars
shards_needed = 3,552,000,000 / 250,000,000 = 14.2 shards
# å‘ä¸Šå–æ•´ â†’ ä¸‹è½½ 16 shardsï¼ˆç•™ä½™é‡ï¼‰

# ============ è®­ç»ƒæ—¶é—´ä¼°ç®— ============
# æ¯ token FLOPs: 6 Ã— 37M = 222M FLOPS
# æ€» FLOPs: 222M Ã— 740M = 1.64e17 FLOPS
# H100 æ€§èƒ½: 989e12 Ã— 8 Ã— 0.4 (MFU) = 3.16e15 FLOPS/s
# æ—¶é—´: 1.64e17 / 3.16e15 = 52 seconds Ã— å®‰å…¨ç³»æ•° â‰ˆ 15 minutes
```

#### ç¤ºä¾‹ 2ï¼šd10 æ¨¡å‹ï¼ˆé€‚åˆå­¦ä¹ å’Œè°ƒè¯•ï¼‰

```python
# ============ æ¨¡å‹æ¶æ„ ============
depth = 10
n_embd = 10 Ã— 64 = 640
n_head = max(1, (640 + 127) // 128) = 5
n_layer = 10
vocab_size = 65536

# ============ å‚æ•°ç»Ÿè®¡ ============
# Transformer å±‚:
#   æ¯å±‚: 12 Ã— 640Â² = 4,915,200 params
#   10 å±‚: 10 Ã— 4,915,200 = 49,152,000 params
# 
# åµŒå…¥å±‚:
#   Token emb: 65536 Ã— 640 = 41,943,040 params
#   LM Head: 640 Ã— 65536 = 41,943,040 params
# 
# æ€»è®¡: 49M + 42M + 42M â‰ˆ 112M params

num_params â‰ˆ 112,000,000

# ============ è®­ç»ƒå‚æ•°ï¼ˆChinchilla 20Ã—ï¼‰============
target_tokens = 20 Ã— 112,000,000 = 2,240,000,000 tokens
num_iterations = 2,240,000,000 / 524,288 = 4,272 steps

# ============ æ•°æ®éœ€æ±‚ ============
chars_per_token = 4.8  # è‹±æ–‡æ•°æ®
total_chars = 2,240,000,000 Ã— 4.8 = 10,752,000,000 chars
shards_needed = 10,752,000,000 / 250,000,000 = 43 shards
# å‘ä¸Šå–æ•´ â†’ ä¸‹è½½ 48 shards

# ============ ç¡¬ä»¶å»ºè®® ============
# å• GPU (ä¾‹å¦‚ RTX 4090):
device_batch_size = 16  # å¯ä»¥è£…ä¸‹
grad_accum_steps = 524,288 / (16 Ã— 2048 Ã— 1) = 16 æ­¥
è®­ç»ƒæ—¶é—´ â‰ˆ 2-3 å°æ—¶ï¼ˆå• GPUï¼‰

# 8 GPU (H100):
device_batch_size = 32
grad_accum_steps = 1
è®­ç»ƒæ—¶é—´ â‰ˆ 15-20 åˆ†é’Ÿ
```

#### ç¤ºä¾‹ 3ï¼šd16 æ¨¡å‹ï¼ˆæ€§ä»·æ¯”ä¹‹é€‰ï¼‰

```python
# ============ æ¨¡å‹æ¶æ„ ============
depth = 16
n_embd = 16 Ã— 64 = 1024
n_head = max(1, (1024 + 127) // 128) = 8
n_layer = 16
vocab_size = 65536

# ============ å‚æ•°ç»Ÿè®¡ ============
num_params â‰ˆ 268,000,000  # çº¦ 268M

# ============ è®­ç»ƒå‚æ•°ï¼ˆChinchilla 20Ã—ï¼‰============
target_tokens = 20 Ã— 268,000,000 = 5,360,000,000 tokens
num_iterations = 5,360,000,000 / 524,288 = 10,223 steps

# ============ æ•°æ®éœ€æ±‚ ============
total_chars = 5,360,000,000 Ã— 4.8 = 25,728,000,000 chars
shards_needed = 25,728,000,000 / 250,000,000 = 103 shards
# å‘ä¸Šå–æ•´ â†’ ä¸‹è½½ 110 shards

# ============ è®­ç»ƒæ—¶é—´å’Œæˆæœ¬ ============
# 8xH100: ~2 å°æ—¶, ~$48
# å• GPU: ~16 å°æ—¶

# ============ æ¨èåœºæ™¯ ============
# - å­¦ä¹  LLM è®­ç»ƒæµç¨‹
# - å¿«é€ŸåŸå‹éªŒè¯
# - æˆæœ¬æ•æ„Ÿçš„é¡¹ç›®
# - ä¸ªäººå­¦ä¹ å’Œç ”ç©¶
```

### 1.4 è®­ç»ƒé…ç½®æ¨è

ä¸åŒè§„æ¨¡æ¨¡å‹çš„æ¨èé…ç½®ï¼š

```python
# ========== d4-d8: è¶…å°æ¨¡å‹ï¼ˆå¿«é€Ÿå®éªŒï¼‰==========
--depth=4                   # æˆ– 6, 8
--device_batch_size=32      # æ˜¾å­˜å……è¶³
--total_batch_size=524288   # ä¿æŒæ ‡å‡†
--max_seq_len=1024          # å¯ç¼©çŸ­ä»¥åŠ é€Ÿ
--num_iterations=-1         # è‡ªåŠ¨è®¡ç®—ï¼ˆChinchillaï¼‰
--eval_every=100            # æ›´é¢‘ç¹è¯„ä¼°
--core_metric_every=500     # æ›´é¢‘ç¹
--sample_every=500

# é€‚ç”¨åœºæ™¯ï¼š
# - ä»£ç è°ƒè¯•
# - è¶…å‚æ•°æœç´¢
# - å• GPU è®­ç»ƒ
# - æ•™å­¦æ¼”ç¤º

# ========== d10-d16: å°æ¨¡å‹ï¼ˆå­¦ä¹ å’ŒåŸå‹ï¼‰==========
--depth=12                  # æˆ– 10, 16
--device_batch_size=32
--total_batch_size=524288
--max_seq_len=2048          # æ ‡å‡†é•¿åº¦
--eval_every=250
--core_metric_every=1000
--sample_every=1000

# é€‚ç”¨åœºæ™¯ï¼š
# - å®Œæ•´æµç¨‹éªŒè¯
# - ç‰¹å®šé¢†åŸŸé€‚é…
# - å¤š GPU è®­ç»ƒå…¥é—¨
# - æˆæœ¬å—é™é¡¹ç›®

# ========== d20-d26: ä¸­ç­‰æ¨¡å‹ï¼ˆå®ç”¨çº§åˆ«ï¼‰==========
--depth=20                  # æˆ– 26
--device_batch_size=32      # d20 å¯ç”¨ 32
--device_batch_size=16      # d26 å»ºè®® 16
--total_batch_size=524288
--max_seq_len=2048
--eval_every=250
--core_metric_every=2000
--sample_every=2000

# é€‚ç”¨åœºæ™¯ï¼š
# - ç”Ÿäº§éƒ¨ç½²ï¼ˆå°è§„æ¨¡ï¼‰
# - ç ”ç©¶é¡¹ç›®
# - å¯¹æ ‡ GPT-2 çº§åˆ«
# - é¢„ç®— $100-$300

# ========== d32+: å¤§æ¨¡å‹ï¼ˆé«˜æ€§èƒ½ï¼‰==========
--depth=32
--device_batch_size=8       # å‡å°ä»¥é€‚åº”æ˜¾å­˜
--total_batch_size=524288
--max_seq_len=2048
--eval_every=250
--core_metric_every=2000

# é€‚ç”¨åœºæ™¯ï¼š
# - é«˜è´¨é‡åº”ç”¨
# - å¯¹æ ‡ GPT-3 å°ç‰ˆæœ¬
# - é¢„ç®— $800+
```

---

## äºŒã€ä¸­æ–‡è¯­æ–™çš„ç‰¹æ®Šæ€§

### 2.1 ä¸­æ–‡ vs è‹±æ–‡çš„å…³é”®å·®å¼‚

```python
# ========== åˆ†è¯æ•ˆç‡å·®å¼‚ ==========
# è‹±æ–‡ï¼ˆFineWeb-Eduï¼‰
chars_per_token = 4.8
# "Hello, world!" â†’ ["Hello", ",", " world", "!"] â†’ 4 tokens â‰ˆ 3.25 chars/token

# ä¸­æ–‡ï¼ˆçº¯ä¸­æ–‡è¯­æ–™ï¼‰
chars_per_token = 1.5 - 2.0  # æ˜¾è‘—æ›´ä½ï¼
# "ä½ å¥½ï¼Œä¸–ç•Œï¼" â†’ ["ä½ ", "å¥½", "ï¼Œ", "ä¸–ç•Œ", "ï¼"] â†’ 5 tokens â‰ˆ 1.2 chars/token

# ä¸­è‹±æ··åˆï¼ˆ50/50ï¼‰
chars_per_token = 2.5 - 3.5
# "Helloä¸–ç•Œ" â†’ ["Hello", "ä¸–", "ç•Œ"] â†’ 3 tokens â‰ˆ 3.3 chars/token

# ========== åŸå› åˆ†æ ==========
# 1. å­—ç¬¦å¯†åº¦
#    - è‹±æ–‡: å¹³å‡å•è¯ 5 å­—ç¬¦ï¼ŒBPE å¸¸åˆå¹¶ä¸º 1-2 tokens
#    - ä¸­æ–‡: å¸¸ç”¨æ±‰å­— ~3000 ä¸ªï¼Œä½† vocab=65536 åªèƒ½è¦†ç›–éƒ¨åˆ†
#    - ç»“æœ: ä¸­æ–‡æ¯ä¸ªå­—å¯èƒ½éœ€è¦ 1-2 tokens

# 2. BPE è®­ç»ƒæ•°æ®
#    - åŸå§‹ nanochat åˆ†è¯å™¨åœ¨è‹±æ–‡æ•°æ®ä¸Šè®­ç»ƒ
#    - å¯¹ä¸­æ–‡çš„ç¼–ç æ•ˆç‡ä½
#    - è§£å†³: éœ€è¦åœ¨ä¸­è‹±æ··åˆæ•°æ®ä¸Šé‡æ–°è®­ç»ƒåˆ†è¯å™¨

# 3. ä¿¡æ¯å¯†åº¦
#    - ä¸­æ–‡æ¯å­—ç¬¦ä¿¡æ¯å¯†åº¦é«˜äºè‹±æ–‡
#    - ç›¸åŒ token æ•°ï¼Œä¸­æ–‡åŒ…å«çš„ä¿¡æ¯é‡å¯èƒ½æ›´å°‘ï¼ˆå¦‚æœç¼–ç ä¸ä¼˜ï¼‰
```

### 2.2 ä¸­æ–‡è¯­æ–™çš„è®­ç»ƒå½±å“

**å½±å“ 1ï¼šæ•°æ®éœ€æ±‚å¢åŠ **

```python
# è‹±æ–‡è®­ç»ƒ d20 æ¨¡å‹
target_tokens = 20 Ã— 561M = 11.2B tokens
total_chars = 11.2B Ã— 4.8 = 53.8B chars
shards_needed = 53.8B / 250M = 215 shards

# çº¯ä¸­æ–‡è®­ç»ƒ d20 æ¨¡å‹ï¼ˆä½¿ç”¨è‹±æ–‡è®­ç»ƒçš„åˆ†è¯å™¨ï¼‰
target_tokens = 20 Ã— 561M = 11.2B tokens  # ç›¸åŒï¼
total_chars = 11.2B Ã— 1.8 = 20.2B chars   # å°‘å¾ˆå¤šå­—ç¬¦
shards_needed = 20.2B / 250M = 81 shards  # éœ€è¦çš„åˆ†ç‰‡æ›´å°‘

# ä½†æ˜¯ï¼å®é™…ä¿¡æ¯é‡å¯èƒ½ä¸è¶³ï¼Œå› ä¸ºï¼š
# - ä¸­æ–‡ç¼–ç æ•ˆç‡ä½ï¼Œæ¯ token åŒ…å«çš„ä¿¡æ¯å°‘
# - å¯èƒ½éœ€è¦æ›´å¤š tokens æ¥è¾¾åˆ°ç›¸åŒçš„è¯­è¨€ç†è§£
```

**å½±å“ 2ï¼šè¯æ±‡è¡¨è¦†ç›–ç‡**

```python
# è‹±æ–‡ï¼ˆBPE on FineWebï¼‰
vocab_size = 65536
è¦†ç›–ç‡ â‰ˆ 99.9%  # å‡ ä¹æ‰€æœ‰è‹±æ–‡éƒ½èƒ½é«˜æ•ˆç¼–ç 

# ä¸­æ–‡ï¼ˆåŒæ ·çš„åˆ†è¯å™¨ï¼‰
å¸¸ç”¨æ±‰å­— = 3000
æ¬¡å¸¸ç”¨æ±‰å­— = 3000
ç”Ÿåƒ»å­— = æ•°ä¸‡
è¦†ç›–ç‡ â‰ˆ 80-90%  # å¾ˆå¤šå­—éœ€è¦ fallback åˆ° byte-level

# é—®é¢˜ï¼š
# - ç”Ÿåƒ»å­—å¯èƒ½è¢«æ‹†æˆå¤šä¸ª byte tokens
# - "ä½ å¥½" å¯èƒ½æ˜¯ ["ä½ ", "å¥½"] è€Œä¸æ˜¯ ["ä½ å¥½"]
# - è®­ç»ƒæ•ˆç‡é™ä½
```

**å½±å“ 3ï¼šåºåˆ—é•¿åº¦æ„ŸçŸ¥**

```python
# ç›¸åŒ token æ•°ä¸‹ï¼Œè¦†ç›–çš„å®é™…å†…å®¹ï¼š

# è‹±æ–‡ (2048 tokens)
å®é™…å­—ç¬¦ â‰ˆ 2048 Ã— 4.8 = 9,830 chars
å®é™…å•è¯ â‰ˆ 9,830 / 5 = 1,966 words
çº¦ 4-5 æ®µè½çš„æ–‡ç« 

# ä¸­æ–‡ (2048 tokens, ç¼–ç æ•ˆç‡ä½)
å®é™…å­—ç¬¦ â‰ˆ 2048 Ã— 1.8 = 3,686 chars
çº¦ 2-3 æ®µè½çš„æ–‡ç« 

# å½±å“ï¼š
# - ä¸­æ–‡æ¨¡å‹çš„"ä¸Šä¸‹æ–‡çª—å£"å®é™…æ›´çŸ­
# - å¯èƒ½éœ€è¦å¢åŠ  max_seq_lenï¼ˆä¾‹å¦‚ 4096ï¼‰
# - ä½†ä¼šå¢åŠ æ˜¾å­˜éœ€æ±‚å’Œè®¡ç®—é‡
```

### 2.3 ä¸­æ–‡åˆ†è¯å™¨çš„å¿…è¦æ€§

**æ–¹æ¡ˆå¯¹æ¯”**ï¼š

| æ–¹æ¡ˆ | chars/token | è®­ç»ƒæ•°æ®éœ€æ±‚ | æ¨¡å‹æ•ˆæœ | æ¨èåº¦ |
|------|-------------|-------------|---------|--------|
| è‹±æ–‡åˆ†è¯å™¨ + ä¸­æ–‡æ•°æ® | 1.5-2.0 | å°‘ | âŒ å·® | â­ |
| ä¸­æ–‡åˆ†è¯å™¨ + ä¸­æ–‡æ•°æ® | 2.5-3.0 | ä¸­ | âœ… å¥½ | â­â­â­â­ |
| ä¸­è‹±æ··åˆåˆ†è¯å™¨ + æ··åˆæ•°æ® | 3.0-3.5 | å¤š | âœ…âœ… å¾ˆå¥½ | â­â­â­â­â­ |
| å¤šè¯­è¨€åˆ†è¯å™¨ (å¦‚ sentencepiece) | 2.0-3.0 | å¤š | âœ… å¥½ | â­â­â­â­ |

---

## ä¸‰ã€ä¸­æ–‡/ä¸­è‹±æ–‡è®­ç»ƒé€‚é…ç­–ç•¥

### 3.1 æ–¹æ¡ˆ 1ï¼šæœ€å°æ”¹åŠ¨ï¼ˆä¸æ¨èï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šå¿«é€Ÿæµ‹è¯•ã€æ¦‚å¿µéªŒè¯

```python
# ========== ä¿æŒåŸæœ‰åˆ†è¯å™¨ ==========
# ä¸é‡æ–°è®­ç»ƒåˆ†è¯å™¨ï¼Œç›´æ¥ç”¨è‹±æ–‡åˆ†è¯å™¨å¤„ç†ä¸­æ–‡

# ========== è°ƒæ•´è®­ç»ƒå‚æ•° ==========
# 1. å¢åŠ è®­ç»ƒ token æ•°ï¼ˆè¡¥å¿ç¼–ç æ•ˆç‡æŸå¤±ï¼‰
target_param_data_ratio = 30  # ä» 20 å¢åŠ åˆ° 30
# d20: 30 Ã— 561M = 16.8B tokensï¼ˆå¢åŠ  50%ï¼‰

# 2. æˆ–å¢åŠ åºåˆ—é•¿åº¦
max_seq_len = 4096  # ä» 2048 ç¿»å€
# ä½†éœ€è¦ç›¸åº”å‡å° device_batch_sizeï¼ˆæ˜¾å­˜é™åˆ¶ï¼‰
device_batch_size = 16  # ä» 32 å‡åŠ

# 3. æ•°æ®å‡†å¤‡
# - æ”¶é›†ä¸­æ–‡æ•°æ®ï¼ˆä¾‹å¦‚ WuDaoCorpora, CLUECorpusï¼‰
# - è½¬æ¢ä¸º Parquet æ ¼å¼
# - æ¯ä¸ªåˆ†ç‰‡ä»ç„¶ 250M chars

# ========== é—®é¢˜ ==========
# âŒ åˆ†è¯æ•ˆç‡ä½ï¼Œæµªè´¹ token é¢„ç®—
# âŒ æ¨¡å‹å­¦ä¹ æ•ˆç‡ä½
# âŒ æœ€ç»ˆæ•ˆæœå·®
# âœ… ä½†å®ç°ç®€å•ï¼Œé€‚åˆå¿«é€ŸéªŒè¯
```

### 3.2 æ–¹æ¡ˆ 2ï¼šé‡æ–°è®­ç»ƒåˆ†è¯å™¨ï¼ˆæ¨èï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šä¸­æ–‡ä¸ºä¸»ã€é«˜è´¨é‡éœ€æ±‚

```python
# ========== ç¬¬ 1 æ­¥ï¼šå‡†å¤‡ä¸­è‹±æ··åˆæ•°æ® ==========
# æ¯”ä¾‹å»ºè®®ï¼š
# - çº¯ä¸­æ–‡é¡¹ç›®: 80% ä¸­æ–‡ + 20% è‹±æ–‡
# - ä¸­è‹±åŒè¯­é¡¹ç›®: 50% ä¸­æ–‡ + 50% è‹±æ–‡
# - ä¸­æ–‡ä¸ºä¸»ä½†éœ€è¦è‹±æ–‡èƒ½åŠ›: 70% ä¸­æ–‡ + 30% è‹±æ–‡

# æ•°æ®æºç¤ºä¾‹ï¼š
chinese_data = [
    "WuDaoCorpora",       # æ‚Ÿé“è¯­æ–™åº“
    "CLUECorpus",         # CLUE ä¸­æ–‡è¯­æ–™
    "Chinese Wikipedia",  # ä¸­æ–‡ç»´åŸº
    "Common Crawl (ä¸­æ–‡)", # è¿‡æ»¤çš„ä¸­æ–‡ç½‘é¡µ
]
english_data = [
    "FineWeb-Edu",        # é«˜è´¨é‡è‹±æ–‡
    "The Pile (å­é›†)",     # å¤šæ ·åŒ–è‹±æ–‡
]

# æ€»é‡ï¼š~5-10B å­—ç¬¦ç”¨äºè®­ç»ƒåˆ†è¯å™¨

# ========== ç¬¬ 2 æ­¥ï¼šè®­ç»ƒåˆ†è¯å™¨ ==========
# ä¿®æ”¹ scripts/tok_train.py

# ä¸‹è½½æ··åˆæ•°æ®
python -m nanochat.dataset -n 20  # ä¸­æ–‡æ•°æ®
# å‡è®¾ä½ æœ‰è‡ªå®šä¹‰çš„ä¸­æ–‡æ•°æ®æº

# è®­ç»ƒåˆ†è¯å™¨ï¼ˆæ··åˆæ•°æ®ï¼‰
python -m scripts.tok_train \
    --max_chars=5000000000 \
    --vocab_size=65536 \
    --data_sources="chinese:80,english:20"  # ä¼ªä»£ç ï¼Œéœ€è¦å®ç°

# é¢„æœŸç»“æœï¼š
# chars_per_token = 2.8 - 3.2ï¼ˆä¸­è‹±æ··åˆï¼‰

# ========== ç¬¬ 3 æ­¥ï¼šè¯„ä¼°åˆ†è¯å™¨ ==========
python -m scripts.tok_eval \
    --test_chinese_file="chinese_test.txt" \
    --test_english_file="english_test.txt"

# æ£€æŸ¥æŒ‡æ ‡ï¼š
# - Chinese chars/token: æœŸæœ› 2.5-3.0
# - English chars/token: æœŸæœ› 4.5-5.0
# - Mixed chars/token: æœŸæœ› 3.0-3.5

# ========== ç¬¬ 4 æ­¥ï¼šè°ƒæ•´è®­ç»ƒå‚æ•° ==========
# ä½¿ç”¨æ–°çš„ chars_per_token é‡æ–°è®¡ç®—æ•°æ®éœ€æ±‚

# d20 æ¨¡å‹ï¼Œå‡è®¾ chars_per_token = 3.0
target_tokens = 20 Ã— 561M = 11.2B tokens
total_chars = 11.2B Ã— 3.0 = 33.6B chars
shards_needed = 33.6B / 250M = 134 shards

# æ¯”åŸç‰ˆè‹±æ–‡ï¼ˆ240 shardsï¼‰å°‘ï¼Œä½†è¿™æ˜¯æ­£å¸¸çš„
# å› ä¸ºä¸­æ–‡ä¿¡æ¯å¯†åº¦æ›´é«˜

# ========== ç¬¬ 5 æ­¥ï¼šæ­£å¸¸è®­ç»ƒ ==========
torchrun --nproc_per_node=8 -m scripts.base_train -- --depth=20
# å…¶ä»–å‚æ•°ä¿æŒä¸å˜
```

### 3.3 æ–¹æ¡ˆ 3ï¼šä¼˜åŒ–è¯æ±‡è¡¨å¤§å°ï¼ˆè¿›é˜¶ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šå†…å­˜å—é™ã€æ¨ç†é€Ÿåº¦ä¼˜å…ˆ

```python
# ========== é—®é¢˜ ==========
# vocab_size = 65536 å¯èƒ½å¯¹ä¸­æ–‡æ¥è¯´è¿‡å¤§ï¼š
# - åµŒå…¥å±‚å‚æ•°: 65536 Ã— n_embd = å¤§é‡å‚æ•°
# - æ¨ç†æ—¶ LM Head è®¡ç®—: éœ€è¦è®¡ç®— 65536 ä¸ª logits

# ========== ä¼˜åŒ–ç­–ç•¥ ==========
# 1. å‡å°è¯æ±‡è¡¨
vocab_size = 32768  # ä» 65536 å‡åŠ

# å½±å“ï¼š
# - å‚æ•°å‡å°‘: çº¦å‡å°‘ 20-30%ï¼ˆåµŒå…¥å±‚å æ¯”å¤§ï¼‰
# - åˆ†è¯æ•ˆç‡ç•¥é™: chars_per_token å¯èƒ½ä» 3.0 é™åˆ° 2.8
# - æ¨ç†åŠ é€Ÿ: LM Head è®¡ç®—å¿« 2 å€
# - ç»¼åˆæƒè¡¡: å¯¹ä¸­æ–‡å¯èƒ½æ›´ä¼˜

# 2. è®­ç»ƒåˆ†è¯å™¨
python -m scripts.tok_train \
    --vocab_size=32768 \
    --max_chars=5000000000 \
    --chinese_ratio=0.7

# 3. é‡æ–°è®¡ç®—æ¨¡å‹å‚æ•°
# d20 with vocab=32768
num_params â‰ˆ 561M - 83M + 42M = 520M
# (å‡å°‘äº†åµŒå…¥å‚æ•°)

# 4. è°ƒæ•´è®­ç»ƒç›®æ ‡
target_tokens = 20 Ã— 520M = 10.4B tokens
# ç•¥å°‘äºåŸç‰ˆ
```

### 3.4 æ–¹æ¡ˆ 4ï¼šå¢åŠ ä¸­æ–‡ç‰¹å®šå¤„ç†ï¼ˆæœ€ä½³å®è·µï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šç”Ÿäº§çº§ä¸­æ–‡ LLM

```python
# ========== å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆ ==========

# 1. åˆ†è¯å™¨ä¼˜åŒ–
# ----------------
# a) ä¸­è‹±æ··åˆè®­ç»ƒï¼ˆ70% ä¸­æ–‡ + 30% è‹±æ–‡ï¼‰
# b) è¯æ±‡è¡¨å¤§å°: 40000-50000ï¼ˆå¹³è¡¡æ•ˆç‡å’Œè¦†ç›–ç‡ï¼‰
# c) æ·»åŠ ä¸­æ–‡ä¸“ç”¨ç‰¹æ®Š token:
SPECIAL_TOKENS += [
    "<|zh_start|>",  # ä¸­æ–‡æ®µè½å¼€å§‹
    "<|en_start|>",  # è‹±æ–‡æ®µè½å¼€å§‹
    "<|code_start|>", # ä»£ç å—å¼€å§‹ï¼ˆå¯¹ä¸­è‹±æ–‡ç¼–ç¨‹éƒ½æœ‰ç”¨ï¼‰
]

# 2. æ•°æ®å¤„ç†ä¼˜åŒ–
# ----------------
# a) é¢„å¤„ç†ä¸­æ–‡æ•°æ®:
#    - ç®€ç¹ä½“è½¬æ¢ï¼ˆç»Ÿä¸€ä¸ºç®€ä½“æˆ–ä¿ç•™ä¸¤è€…ï¼‰
#    - å…¨è§’åŠè§’æ ‡å‡†åŒ–
#    - æ¸…æ´—ä½è´¨é‡ç½‘é¡µæ•°æ®

# b) æ•°æ®æ··åˆæ¯”ä¾‹:
#    - é€šç”¨ä¸­æ–‡: 40%
#    - é€šç”¨è‹±æ–‡: 30%
#    - ä¸­æ–‡ä¹¦ç±/æ–‡ç« : 15%
#    - è‹±æ–‡æŠ€æœ¯æ–‡æ¡£: 10%
#    - ä»£ç ï¼ˆä¸­è‹±æ–‡æ³¨é‡Šï¼‰: 5%

# 3. æ¨¡å‹è®­ç»ƒè°ƒæ•´
# ----------------
# a) å¢åŠ è®­ç»ƒ tokenï¼ˆä¸­æ–‡ä¿¡æ¯å¯†åº¦é«˜ï¼‰
target_param_data_ratio = 25  # ä» 20 å¢åŠ åˆ° 25
# ç†ç”±ï¼šè™½ç„¶åˆ†è¯å™¨ä¼˜åŒ–äº†ï¼Œä½†ä¸­æ–‡æ¯ token ä¿¡æ¯é‡ä»é«˜äºè‹±æ–‡

# b) è€ƒè™‘å¢åŠ åºåˆ—é•¿åº¦ï¼ˆå¯é€‰ï¼‰
max_seq_len = 3072  # ä» 2048 å¢åŠ  50%
# ç†ç”±ï¼šä¸­æ–‡è¯­å¢ƒä¾èµ–æ€§å¼ºï¼Œæ›´é•¿ä¸Šä¸‹æ–‡æœ‰å¸®åŠ©
# ä»£ä»·ï¼šæ˜¾å­˜éœ€æ±‚å¢åŠ ï¼Œéœ€å‡å° device_batch_size

# c) å­¦ä¹ ç‡å¯èƒ½éœ€è¦å¾®è°ƒ
# ä¸­æ–‡è®­ç»ƒå¯èƒ½éœ€è¦ç•¥é«˜çš„å­¦ä¹ ç‡ï¼ˆç»éªŒå€¼ï¼‰
matrix_lr = 0.022  # ä» 0.02 ç•¥å¾®å¢åŠ 

# 4. è¯„ä¼°ä½“ç³»è°ƒæ•´
# ----------------
# a) æ·»åŠ ä¸­æ–‡è¯„ä¼°ä»»åŠ¡:
from tasks.c_eval import C_EVAL  # å‡è®¾å®ç°
from tasks.cmmlu import CMMLU

train_tasks = [
    C_EVAL(subset="all"),        # ä¸­æ–‡ç»¼åˆè¯„ä¼°
    CMMLU(subset="all"),         # ä¸­æ–‡ MMLU
    ARC(...),                    # ä¿ç•™è‹±æ–‡ä»»åŠ¡
    GSM8K(...),
]

# b) ä¸­æ–‡ç‰¹å®šæŒ‡æ ‡:
#    - ä¸­æ–‡è¯­ä¹‰ç†è§£
#    - å¤è¯—è¯ç”Ÿæˆ
#    - æˆè¯­ä½¿ç”¨
#    - ä¸­è‹±ç¿»è¯‘

# 5. å®Œæ•´è®­ç»ƒè„šæœ¬
# ----------------
# speedrun_chinese.sh

#!/bin/bash

# ç¯å¢ƒè®¾ç½®
export NANOCHAT_BASE_DIR="$HOME/.cache/nanochat_chinese"
mkdir -p $NANOCHAT_BASE_DIR

# ä¸‹è½½ä¸­æ–‡æ•°æ®ï¼ˆå‡è®¾å·²å‡†å¤‡ï¼‰
python -m scripts.download_chinese_data -n 300

# è®­ç»ƒåˆ†è¯å™¨ï¼ˆä¸­è‹±æ··åˆï¼‰
python -m scripts.tok_train \
    --vocab_size=40000 \
    --max_chars=5000000000 \
    --chinese_ratio=0.7 \
    --english_ratio=0.3

# é¢„è®­ç»ƒ
torchrun --nproc_per_node=8 -m scripts.base_train -- \
    --depth=20 \
    --target_param_data_ratio=25 \
    --max_seq_len=3072 \
    --device_batch_size=16 \
    --matrix_lr=0.022 \
    --run=chinese_d20

# ä¸­é—´è®­ç»ƒï¼ˆä¸­æ–‡å¯¹è¯æ•°æ®ï¼‰
torchrun --nproc_per_node=8 -m scripts.mid_train -- \
    --source=base \
    --chinese_data="chinese_conversations.jsonl"

# SFTï¼ˆä¸­è‹±æ··åˆä»»åŠ¡ï¼‰
torchrun --nproc_per_node=8 -m scripts.chat_sft -- \
    --source=mid \
    --use_chinese_tasks=True

# è¯„ä¼°
python -m scripts.eval_chinese
```

---

## å››ã€å®æˆ˜é…ç½®å»ºè®®

### 4.1 åœºæ™¯ 1ï¼šçº¯ä¸­æ–‡é¡¹ç›®ï¼ˆd16 æ¨¡å‹ï¼‰

**ç›®æ ‡**ï¼šæˆæœ¬ ~$50ï¼Œè®­ç»ƒæ—¶é—´ 2-3 å°æ—¶

```bash
# ========== æ­¥éª¤ 1ï¼šå‡†å¤‡æ•°æ® ==========
# æ”¶é›†ä¸­æ–‡æ•°æ®ï¼ˆæ¨èæ¥æºï¼‰ï¼š
# - ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼šé«˜è´¨é‡ï¼Œçº¦ 2GB
# - æ‚Ÿé“è¯­æ–™åº“ï¼ˆå­é›†ï¼‰ï¼šé€šç”¨ä¸­æ–‡ï¼Œé€‰æ‹© 20-30GB
# - ä¸­æ–‡æ–°é—»ï¼šæ—¶æ•ˆæ€§å¼ºï¼Œé€‰æ‹© 5-10GB
# - ä¸­æ–‡ä¹¦ç±ï¼šæ·±åº¦å†…å®¹ï¼Œé€‰æ‹© 5-10GB

# æ€»è®¡ï¼š30-50GB åŸå§‹æ–‡æœ¬
# æ¸…æ´—åï¼š20-30GB é«˜è´¨é‡æ–‡æœ¬
# åˆ†ç‰‡åŒ–ï¼š120-150 ä¸ª 250MB åˆ†ç‰‡

# ========== æ­¥éª¤ 2ï¼šè®­ç»ƒåˆ†è¯å™¨ ==========
python -m scripts.tok_train \
    --vocab_size=40000 \
    --max_chars=5000000000 \
    --data_path="chinese_data/"

# è¯„ä¼°ç»“æœï¼š
# chars_per_token æœŸæœ› 2.5-3.0

# ========== æ­¥éª¤ 3ï¼šè®¡ç®—è®­ç»ƒå‚æ•° ==========
# d16 æ¨¡å‹
depth = 16
num_params = 268M

# è°ƒæ•´åçš„å‚æ•°
vocab_size = 40000  # å‡å°è¯æ±‡è¡¨
num_params = 268M - 60M + 30M = 238M  # é‡æ–°è®¡ç®—

# è®­ç»ƒ token æ•°ï¼ˆå¢åŠ åˆ° 25Ã—ï¼‰
target_tokens = 25 Ã— 238M = 5.95B tokens

# æ•°æ®éœ€æ±‚
chars_per_token = 2.8
total_chars = 5.95B Ã— 2.8 = 16.66B chars
shards_needed = 16.66B / 250M = 67 shards

# ========== æ­¥éª¤ 4ï¼šè®­ç»ƒ ==========
torchrun --nproc_per_node=8 -m scripts.base_train -- \
    --depth=16 \
    --target_param_data_ratio=25 \
    --device_batch_size=32 \
    --total_batch_size=524288 \
    --max_seq_len=2048 \
    --run=chinese_d16

# é¢„æœŸï¼š
# - è®­ç»ƒæ—¶é—´ï¼š~2.5 å°æ—¶
# - æˆæœ¬ï¼š~$60ï¼ˆ8xH100 @ $24/hrï¼‰
# - æ•ˆæœï¼šæ¥è¿‘ä¸­æ–‡ GPT-2 æ°´å¹³

# ========== æ­¥éª¤ 5ï¼šè¯„ä¼° ==========
# æ·»åŠ ä¸­æ–‡è¯„ä¼°ï¼ˆéœ€è¦è‡ªå·±å®ç°æˆ–ä½¿ç”¨å¼€æºå·¥å…·ï¼‰
# é¢„æœŸæŒ‡æ ‡ï¼š
# - C-EVALï¼ˆä¸­å­¦/å¤§å­¦è€ƒè¯•ï¼‰ï¼š30-40%
# - ä¸­æ–‡å›°æƒ‘åº¦ï¼š15-20
# - å¯¹è¯æµç•…åº¦ï¼šä¸­ç­‰
```

### 4.2 åœºæ™¯ 2ï¼šä¸­è‹±åŒè¯­é¡¹ç›®ï¼ˆd20 æ¨¡å‹ï¼‰

**ç›®æ ‡**ï¼šåŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼Œé¢„ç®— ~$150

```bash
# ========== æ•°æ®æ¯”ä¾‹ ==========
Chinese data: 50% (30GB)
English data: 50% (30GB)
Total: 60GB â†’ ~240 shards

# ========== åˆ†è¯å™¨è®­ç»ƒ ==========
python -m scripts.tok_train \
    --vocab_size=50000 \
    --max_chars=8000000000 \
    --chinese_data="chinese_corpus/" \
    --english_data="english_corpus/" \
    --mix_ratio=0.5

# é¢„æœŸï¼š
# - Chinese chars/token: 2.8-3.2
# - English chars/token: 4.5-5.0
# - Mixed chars/token: 3.5-3.8

# ========== è®­ç»ƒå‚æ•° ==========
depth = 20
vocab_size = 50000
num_params â‰ˆ 530M  # ç•¥å°‘äºåŸç‰ˆ

target_param_data_ratio = 25
target_tokens = 25 Ã— 530M = 13.25B tokens

chars_per_token = 3.6  # æ··åˆæ•°æ®
total_chars = 13.25B Ã— 3.6 = 47.7B chars
shards_needed = 47.7B / 250M = 191 shards

# ========== è®­ç»ƒå‘½ä»¤ ==========
torchrun --nproc_per_node=8 -m scripts.base_train -- \
    --depth=20 \
    --target_param_data_ratio=25 \
    --device_batch_size=32 \
    --max_seq_len=2048 \
    --run=bilingual_d20

# ========== ä¸­é—´è®­ç»ƒï¼ˆåŒè¯­å¯¹è¯ï¼‰==========
# å‡†å¤‡ä¸­è‹±æ–‡å¯¹è¯æ•°æ®
# æ¯”ä¾‹ï¼š60% ä¸­æ–‡å¯¹è¯ + 40% è‹±æ–‡å¯¹è¯

torchrun --nproc_per_node=8 -m scripts.mid_train -- \
    --source=base \
    --chinese_conv="chinese_conversations.jsonl" \
    --english_conv="english_conversations.jsonl"

# ========== SFTï¼ˆåŒè¯­ä»»åŠ¡ï¼‰==========
train_ds = TaskMixture([
    # è‹±æ–‡ä»»åŠ¡
    ARC(subset="ARC-Easy", split="train"),
    GSM8K(subset="main", split="train"),
    SmolTalk(split="train", stop=5000),
    
    # ä¸­æ–‡ä»»åŠ¡
    C_EVAL(split="train"),           # å‡è®¾å®ç°
    ChineseGSM8K(split="train"),     # ä¸­æ–‡æ•°å­¦
    ChineseConversations(n=5000),    # ä¸­æ–‡å¯¹è¯
    
    # ç¿»è¯‘ä»»åŠ¡ï¼ˆå¢å¼ºåŒè¯­èƒ½åŠ›ï¼‰
    TranslationTask(pairs=1000),     # ä¸­è‹±äº’è¯‘
])

torchrun --nproc_per_node=8 -m scripts.chat_sft -- \
    --source=mid \
    --use_bilingual_tasks=True

# ========== é¢„æœŸæ•ˆæœ ==========
# è‹±æ–‡èƒ½åŠ›ï¼š
# - CORE: ~0.22
# - GSM8K: ~0.04

# ä¸­æ–‡èƒ½åŠ›ï¼š
# - C-EVAL: ~0.35
# - ä¸­æ–‡å¯¹è¯: æµç•…

# ç¿»è¯‘èƒ½åŠ›ï¼š
# - åŸºæœ¬çš„ä¸­è‹±äº’è¯‘ï¼ˆä¸å®Œç¾ä½†å¯ç”¨ï¼‰

# æ€»æˆæœ¬ï¼š~$150ï¼ˆé¢„è®­ç»ƒ 4hr + ä¸­é—´è®­ç»ƒ 1hr + SFT 1hrï¼‰
```

### 4.3 åœºæ™¯ 3ï¼šå°è§„æ¨¡ä¸­æ–‡å®éªŒï¼ˆd8 æ¨¡å‹ï¼Œå• GPUï¼‰

**ç›®æ ‡**ï¼šä¸ªäººå­¦ä¹ ï¼Œå• GPU è®­ç»ƒï¼Œé¢„ç®— ~$0ï¼ˆä½¿ç”¨ä¸ªäºº GPUï¼‰

```bash
# ========== ç¡¬ä»¶ ==========
# RTX 4090 æˆ– RTX 3090ï¼ˆ24GB VRAMï¼‰

# ========== æ•°æ®å‡†å¤‡ ==========
# æ”¶é›†å°è§„æ¨¡ä¸­æ–‡æ•°æ®ï¼š
# - ä¸­æ–‡ç»´åŸºï¼š~2GB
# - ç²¾é€‰æ–°é—»ï¼š~1GB
# - ä¹¦ç±æ‘˜å½•ï¼š~1GB
# æ€»è®¡ï¼š~4GB â†’ 16 shards

# ========== åˆ†è¯å™¨ï¼ˆç®€åŒ–ï¼‰==========
# æ–¹æ¡ˆ Aï¼šä½¿ç”¨ç°æœ‰ä¸­æ–‡åˆ†è¯å™¨ï¼ˆå¦‚ BertTokenizerï¼‰
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
vocab_size = 21128

# æ–¹æ¡ˆ Bï¼šå¿«é€Ÿè®­ç»ƒç®€å•åˆ†è¯å™¨
python -m scripts.tok_train \
    --vocab_size=30000 \
    --max_chars=1000000000 \
    --data_path="small_chinese_data/"

# ========== æ¨¡å‹é…ç½® ==========
depth = 8
vocab_size = 30000  # å‡å°
num_params â‰ˆ 70M  # å°æ¨¡å‹

target_param_data_ratio = 20  # æ ‡å‡†
target_tokens = 20 Ã— 70M = 1.4B tokens

chars_per_token = 2.5
total_chars = 1.4B Ã— 2.5 = 3.5B chars
shards_needed = 3.5B / 250M = 14 shards

# ========== è®­ç»ƒå‘½ä»¤ï¼ˆå• GPUï¼‰==========
python -m scripts.base_train \
    --depth=8 \
    --device_batch_size=8 \
    --total_batch_size=131072 \
    --max_seq_len=1024 \
    --target_param_data_ratio=20 \
    --num_iterations=10000 \
    --eval_every=100 \
    --core_metric_every=1000

# æ³¨æ„ï¼š
# - æ—  torchrunï¼ˆå• GPUï¼‰
# - device_batch_size=8ï¼ˆé€‚åº”æ˜¾å­˜ï¼‰
# - total_batch_size=131072ï¼ˆå‡å°ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰
# - max_seq_len=1024ï¼ˆå‡åŠï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
# - grad_accum_steps = 131072 / (8Ã—1024Ã—1) = 16 æ­¥

# ========== è®­ç»ƒæ—¶é—´ ==========
# å• RTX 4090ï¼š
# - æ¯æ­¥çº¦ 200msï¼ˆåŒ…æ‹¬æ¢¯åº¦ç´¯ç§¯ï¼‰
# - 10,000 æ­¥ = 2,000 ç§’ = 33 åˆ†é’Ÿ
# - åŠ ä¸Šè¯„ä¼°ç­‰ â†’ çº¦ 1 å°æ—¶

# ========== ç®€åŒ–æµç¨‹ï¼ˆè·³è¿‡ Midtrainingï¼‰==========
# ç›´æ¥ä» base åˆ° SFTï¼Œé€‚åˆå°æ¨¡å‹

# å‡†å¤‡å°‘é‡ä¸­æ–‡å¯¹è¯æ•°æ®ï¼ˆ~1000 æ¡ï¼‰
python -m scripts.chat_sft \
    --source=base \
    --device_batch_size=4 \
    --num_epochs=3 \
    --chinese_only=True

# ========== é¢„æœŸæ•ˆæœ ==========
# - èƒ½å¤Ÿç”ŸæˆåŸºæœ¬çš„ä¸­æ–‡å¥å­
# - å¯¹è¯èƒ½åŠ›æœ‰é™ï¼ˆæ¨¡å‹å¤ªå°ï¼‰
# - é€‚åˆå­¦ä¹ å’Œç†è§£è®­ç»ƒæµç¨‹
# - ä¸é€‚åˆå®é™…åº”ç”¨
```

### 4.4 å…³é”®å‚æ•°å¯¹ç…§è¡¨ï¼ˆä¸­æ–‡ vs è‹±æ–‡ï¼‰

| å‚æ•° | è‹±æ–‡è¯­æ–™ | ä¸­æ–‡è¯­æ–™ | ä¸­è‹±æ··åˆï¼ˆ50/50ï¼‰| å»ºè®®è°ƒæ•´ |
|------|---------|---------|----------------|---------|
| `chars_per_token` | 4.8 | 1.8-2.5 | 3.0-3.5 | N/Aï¼ˆæµ‹é‡å€¼ï¼‰|
| `vocab_size` | 65536 | 40000-50000 | 50000-60000 | âœ… ä¸­æ–‡å‡å° |
| `target_param_data_ratio` | 20 | 25-30 | 22-25 | âœ… ä¸­æ–‡å¢åŠ  |
| `max_seq_len` | 2048 | 3072-4096 | 2048-3072 | å¯é€‰å¢åŠ  |
| `device_batch_size` | 32 | 16-24 | 24-32 | å¦‚æœå¢åŠ  seq_len |
| `total_batch_size` | 524288 | 524288 | 524288 | â­• ä¿æŒä¸å˜ |
| `matrix_lr` | 0.02 | 0.022-0.025 | 0.02-0.022 | å¯é€‰å¾®è°ƒ |
| è®­ç»ƒæ—¶é—´ï¼ˆd20@8xH100ï¼‰| 4h | 5-6h | 4.5-5h | ä¿¡æ¯é‡å¢åŠ  |

---

## äº”ã€æ€»ç»“ä¸æœ€ä½³å®è·µ

### 5.1 æ ¸å¿ƒè¦ç‚¹

1. **æ¨¡å‹è§„æ¨¡é€‰æ‹©**ï¼š
   - d4-d8: å¿«é€Ÿå®éªŒã€å• GPU è®­ç»ƒï¼ˆ$0-$20ï¼‰
   - d10-d16: å­¦ä¹ å’ŒåŸå‹ã€å¤š GPU å…¥é—¨ï¼ˆ$20-$60ï¼‰
   - d20-d26: å®ç”¨çº§åˆ«ã€å¯¹æ ‡ GPT-2ï¼ˆ$100-$300ï¼‰
   - d32+: é«˜æ€§èƒ½ã€å¯¹æ ‡ GPT-3 å°ç‰ˆæœ¬ï¼ˆ$800+ï¼‰

2. **ä¸­æ–‡è®­ç»ƒå¿…åš**ï¼š
   - âœ… é‡æ–°è®­ç»ƒåˆ†è¯å™¨ï¼ˆä¸­è‹±æ··åˆæ•°æ®ï¼‰
   - âœ… è°ƒæ•´è¯æ±‡è¡¨å¤§å°ï¼ˆ40k-50k vs 65kï¼‰
   - âœ… å¢åŠ è®­ç»ƒ token æ¯”ä¾‹ï¼ˆ25Ã— vs 20Ã—ï¼‰
   - âœ… æ·»åŠ ä¸­æ–‡è¯„ä¼°ä»»åŠ¡

3. **ä¸­æ–‡è®­ç»ƒå¯é€‰**ï¼š
   - å¢åŠ åºåˆ—é•¿åº¦ï¼ˆ2048 â†’ 3072ï¼‰
   - å¾®è°ƒå­¦ä¹ ç‡ï¼ˆ0.02 â†’ 0.022ï¼‰
   - è°ƒæ•´æ•°æ®æ··åˆæ¯”ä¾‹

4. **å¸¸è§è¯¯åŒº**ï¼š
   - âŒ ç›´æ¥ç”¨è‹±æ–‡åˆ†è¯å™¨è®­ç»ƒä¸­æ–‡ï¼ˆæ•ˆæœå¾ˆå·®ï¼‰
   - âŒ å¿½è§† chars_per_token å·®å¼‚ï¼ˆæ•°æ®å‡†å¤‡å‡ºé”™ï¼‰
   - âŒ ç”¨ç›¸åŒçš„ token æ•°è®­ç»ƒä¸­è‹±æ–‡ï¼ˆä¸­æ–‡éœ€è¦æ›´å¤šï¼‰
   - âŒ ä¸è¯„ä¼°ä¸­æ–‡ç‰¹å®šèƒ½åŠ›ï¼ˆåªçœ‹è‹±æ–‡æŒ‡æ ‡ï¼‰

### 5.2 å¿«é€Ÿå†³ç­–æ ‘

```
æ˜¯å¦æœ‰ GPU èµ„æºï¼Ÿ
â”œâ”€ æ˜¯ â†’ é¢„ç®—å¤šå°‘ï¼Ÿ
â”‚   â”œâ”€ <$50 â†’ d10-d16ï¼Œå•/å¤š GPUï¼Œç®€åŒ–æµç¨‹
â”‚   â”œâ”€ $50-$150 â†’ d16-d20ï¼Œ8 GPUï¼Œæ ‡å‡†æµç¨‹
â”‚   â””â”€ >$150 â†’ d20-d32ï¼Œ8 GPUï¼Œå®Œæ•´æµç¨‹
â”‚
â””â”€ å¦ â†’ 
    â”œâ”€ äº‘æœåŠ¡ï¼ˆLambda/RunPodï¼‰â†’ æŒ‰éœ€ç§Ÿç”¨
    â””â”€ CPU è®­ç»ƒ â†’ ä»… d4-d6ï¼Œå­¦ä¹ ç”¨

è¯­æ–™ç±»å‹ï¼Ÿ
â”œâ”€ çº¯ä¸­æ–‡ â†’ 70-80% ä¸­æ–‡æ•°æ®ï¼Œvocab=40kï¼Œratio=25
â”œâ”€ çº¯è‹±æ–‡ â†’ 100% è‹±æ–‡æ•°æ®ï¼Œvocab=65kï¼Œratio=20
â””â”€ ä¸­è‹±æ··åˆ â†’ 50/50 æ··åˆï¼Œvocab=50kï¼Œratio=22

åº”ç”¨åœºæ™¯ï¼Ÿ
â”œâ”€ å­¦ä¹ /ç ”ç©¶ â†’ d8-d16ï¼Œç®€åŒ–è¯„ä¼°
â”œâ”€ åŸå‹éªŒè¯ â†’ d16-d20ï¼Œæ ‡å‡†æµç¨‹
â””â”€ ç”Ÿäº§éƒ¨ç½² â†’ d20-d32ï¼Œå®Œæ•´æµç¨‹ + ä¼˜åŒ–
```

### 5.3 å®ç”¨å»ºè®®

**å»ºè®® 1ï¼šä»å°å¼€å§‹**
```bash
# ç¬¬ä¸€æ¬¡è®­ç»ƒï¼šd8 æˆ– d10
# ç›®æ ‡ï¼šéªŒè¯æ•°æ®ç®¡é“ã€åˆ†è¯å™¨ã€è®­ç»ƒè„šæœ¬
# æ—¶é—´ï¼š1-2 å°æ—¶
# æˆæœ¬ï¼š$20-30

# ç¬¬äºŒæ¬¡è®­ç»ƒï¼šd16
# ç›®æ ‡ï¼šå®Œæ•´æµç¨‹ã€è¯„ä¼°ä½“ç³»
# æ—¶é—´ï¼š3-4 å°æ—¶
# æˆæœ¬ï¼š$60-80

# ç¬¬ä¸‰æ¬¡è®­ç»ƒï¼šd20+
# ç›®æ ‡ï¼šé«˜è´¨é‡æ¨¡å‹ã€ç”Ÿäº§éƒ¨ç½²
# æ—¶é—´ï¼š6+ å°æ—¶
# æˆæœ¬ï¼š$150+
```

**å»ºè®® 2ï¼šåˆ†è¯å™¨ä¼˜å…ˆ**
```bash
# å…ˆæŠ•å…¥æ—¶é—´ä¼˜åŒ–åˆ†è¯å™¨ï¼š
# 1. æ”¶é›†é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼ˆ5-10GBï¼‰
# 2. å¤šæ¬¡å®éªŒ vocab_sizeï¼ˆ30k, 40k, 50kï¼‰
# 3. è¯„ä¼° chars_per_token å’Œè¦†ç›–ç‡
# 4. é€‰æ‹©æœ€ä½³é…ç½®

# å¥½çš„åˆ†è¯å™¨ = è®­ç»ƒæˆåŠŸçš„ä¸€åŠ
```

**å»ºè®® 3ï¼šç›‘æ§å…³é”®æŒ‡æ ‡**
```python
# è®­ç»ƒä¸­ç›‘æ§ï¼š
# - è®­ç»ƒæŸå¤±ä¸‹é™ï¼ˆå¿…é¡»ï¼‰
# - éªŒè¯æŸå¤±ä¸‹é™ï¼ˆå¿…é¡»ï¼‰
# - chars_per_token åŒ¹é…é¢„æœŸï¼ˆé‡è¦ï¼‰
# - ä¸­è‹±æ–‡æ ·æœ¬è´¨é‡ï¼ˆå®šæœŸæ£€æŸ¥ï¼‰
# - MFU > 30%ï¼ˆæ•ˆç‡æŒ‡æ ‡ï¼‰

# è¯„ä¼°æ—¶æ£€æŸ¥ï¼š
# - ä¸­æ–‡å’Œè‹±æ–‡çš„ CORE/ç±»ä¼¼æŒ‡æ ‡
# - å¯¹è¯æµç•…åº¦ï¼ˆäººå·¥è¯„ä¼°ï¼‰
# - ç‰¹å®šä»»åŠ¡è¡¨ç°ï¼ˆæ•°å­¦ã€ç¿»è¯‘ç­‰ï¼‰
```

**å»ºè®® 4ï¼šä¿å­˜ä¸­é—´ç»“æœ**
```bash
# æ¯ä¸ªé˜¶æ®µä¿å­˜æ£€æŸ¥ç‚¹ï¼š
# - åˆ†è¯å™¨ â†’ tokenizer/
# - åŸºç¡€æ¨¡å‹ â†’ base_checkpoints/
# - ä¸­é—´è®­ç»ƒ â†’ chatmid_checkpoints/
# - SFT â†’ chatsft_checkpoints/

# å¦‚æœæŸä¸ªé˜¶æ®µå¤±è´¥ï¼Œå¯ä»¥å›é€€é‡æ¥
# é¿å…ä»å¤´è®­ç»ƒ
```

---

## é™„å½•ï¼šå®Œæ•´é…ç½®ç¤ºä¾‹

### A.1 d16 ä¸­æ–‡æ¨¡å‹å®Œæ•´é…ç½®

```bash
# speedrun_chinese_d16.sh

#!/bin/bash
set -e

# ç¯å¢ƒå˜é‡
export NANOCHAT_BASE_DIR="$HOME/.cache/nanochat_chinese"
export OMP_NUM_THREADS=1
mkdir -p $NANOCHAT_BASE_DIR

# Python ç¯å¢ƒ
uv venv && source .venv/bin/activate
uv sync --extra gpu

# Rust ç¯å¢ƒ
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source "$HOME/.cargo/env"
uv run maturin develop --release --manifest-path rustbpe/Cargo.toml

# ========== æ•°æ®å‡†å¤‡ ==========
echo "Downloading Chinese data..."
python -m scripts.download_chinese_data -n 150

# ========== åˆ†è¯å™¨è®­ç»ƒ ==========
echo "Training tokenizer..."
python -m scripts.tok_train \
    --vocab_size=40000 \
    --max_chars=5000000000 \
    --data_path="$NANOCHAT_BASE_DIR/chinese_data/" \
    --chinese_ratio=0.75 \
    --english_ratio=0.25

python -m scripts.tok_eval

# ========== é¢„è®­ç»ƒ ==========
echo "Pretraining base model..."
torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=16 \
    --target_param_data_ratio=25 \
    --device_batch_size=32 \
    --total_batch_size=524288 \
    --max_seq_len=2048 \
    --matrix_lr=0.022 \
    --eval_every=200 \
    --core_metric_every=1000 \
    --sample_every=1000 \
    --run=chinese_d16_base

torchrun --standalone --nproc_per_node=8 -m scripts.base_loss
torchrun --standalone --nproc_per_node=8 -m scripts.base_eval

# ========== ä¸­é—´è®­ç»ƒ ==========
echo "Midtraining..."
curl -L -o $NANOCHAT_BASE_DIR/chinese_conversations.jsonl \
    https://your-server.com/chinese_conversations.jsonl

torchrun --standalone --nproc_per_node=8 -m scripts.mid_train -- \
    --source=base \
    --model_tag=d16 \
    --run=chinese_d16_mid

torchrun --standalone --nproc_per_node=8 -m scripts.chat_eval -- -i mid

# ========== ç›‘ç£å¾®è°ƒ ==========
echo "Supervised finetuning..."
torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- \
    --source=mid \
    --model_tag=d16 \
    --num_epochs=1 \
    --run=chinese_d16_sft

torchrun --standalone --nproc_per_node=8 -m scripts.chat_eval -- -i sft

# ========== ç”ŸæˆæŠ¥å‘Š ==========
python -m nanochat.report generate

echo "Training complete! Total time: ~3-4 hours, Cost: ~$72-96"
echo "Model saved to: $NANOCHAT_BASE_DIR/chatsft_checkpoints/d16/"
echo "Report: $NANOCHAT_BASE_DIR/report/report.md"
```

---

**æ–‡æ¡£ç»“æŸ**

å¸Œæœ›è¿™ä»½æŒ‡å—èƒ½å¸®åŠ©ä½ æˆåŠŸè®­ç»ƒä¸­æ–‡æˆ–ä¸­è‹±æ–‡åŒè¯­çš„ nanochat æ¨¡å‹ï¼ğŸš€ğŸ‡¨ğŸ‡³

å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿åœ¨ GitHub Discussions è®¨è®ºï¼

