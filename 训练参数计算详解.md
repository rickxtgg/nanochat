# nanochat è®­ç»ƒå‚æ•°è®¡ç®—è¯¦è§£

> æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Š nanochat ä¸­å„ä¸ªè®­ç»ƒå‚æ•°æ˜¯å¦‚ä½•è®¡ç®—å’Œç›¸äº’å½±å“çš„

---

## ç›®å½•

- [ä¸€ã€æ ¸å¿ƒæ¦‚å¿µ](#ä¸€æ ¸å¿ƒæ¦‚å¿µ)
- [äºŒã€å‚æ•°è®¡ç®—æµç¨‹](#äºŒå‚æ•°è®¡ç®—æµç¨‹)
- [ä¸‰ã€æ•°æ®é›†ç›¸å…³å‚æ•°](#ä¸‰æ•°æ®é›†ç›¸å…³å‚æ•°)
- [å››ã€æ‰¹æ¬¡å¤§å°å‚æ•°è¯¦è§£](#å››æ‰¹æ¬¡å¤§å°å‚æ•°è¯¦è§£)
- [äº”ã€ä¸åŒè®­ç»ƒé˜¶æ®µçš„å‚æ•°ç­–ç•¥](#äº”ä¸åŒè®­ç»ƒé˜¶æ®µçš„å‚æ•°ç­–ç•¥)
- [å…­ã€å®é™…è®¡ç®—ç¤ºä¾‹](#å…­å®é™…è®¡ç®—ç¤ºä¾‹)

---

## ä¸€ã€æ ¸å¿ƒæ¦‚å¿µ

### 1.1 å‚æ•°å±‚çº§å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è®­ç»ƒè§„æ¨¡æ§åˆ¶å‚æ•°                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚           â”‚           â”‚
          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ æ¨¡å‹è§„æ¨¡  â”‚ â”‚æ‰¹æ¬¡å¤§å°â”‚ â”‚è®­ç»ƒæŒç»­æ—¶é—´  â”‚
          â”‚ (depth)  â”‚ â”‚(batch) â”‚ â”‚(iterations)  â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚           â”‚           â”‚
                â–¼           â–¼           â–¼
        å†³å®šå‚æ•°æ•°é‡    å†³å®šå¹¶è¡Œåº¦   å†³å®šæ€»tokenæ•°
                â”‚           â”‚           â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    æ‰€éœ€æ•°æ®é›†å¤§å°
```

### 1.2 å…³é”®å‚æ•°åˆ†ç±»

**å›ºå®šå‚æ•°ï¼ˆè®¾è®¡é€‰æ‹©ï¼Œé€šå¸¸ä¸å˜ï¼‰**ï¼š
- `total_batch_size = 524,288`  # æ€»æ‰¹æ¬¡å¤§å°ï¼ˆtokensï¼‰
- `max_seq_len = 2048`           # åºåˆ—é•¿åº¦
- `target_param_data_ratio = 20` # Chinchilla æ¯”ä¾‹

**è‡ªåŠ¨è®¡ç®—å‚æ•°ï¼ˆæ ¹æ®æ¨¡å‹è§„æ¨¡ï¼‰**ï¼š
- `model_dim = depth Ã— 64`       # æ¨¡å‹ç»´åº¦
- `num_heads = ceil(model_dim / 128)` # æ³¨æ„åŠ›å¤´æ•°
- `num_params`                   # å‚æ•°æ€»æ•°ï¼ˆæ ¹æ®æ¨¡å‹ç»“æ„è®¡ç®—ï¼‰

**ç¯å¢ƒç›¸å…³å‚æ•°ï¼ˆæ ¹æ®ç¡¬ä»¶ï¼‰**ï¼š
- `ddp_world_size = 8`           # GPU æ•°é‡
- `device_batch_size = 32`       # æ¯ GPU æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰

**æ´¾ç”Ÿå‚æ•°ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰**ï¼š
- `grad_accum_steps`             # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- `num_iterations`               # è®­ç»ƒæ­¥æ•°
- `total_tokens`                 # æ€»è®­ç»ƒ token æ•°

---

## äºŒã€å‚æ•°è®¡ç®—æµç¨‹

### 2.1 å®Œæ•´è®¡ç®—é“¾

```python
# ============ é˜¶æ®µ 1ï¼šç”¨æˆ·è¾“å…¥ ============
depth = 20                      # ç”¨æˆ·æŒ‡å®šï¼šæ¨¡å‹æ·±åº¦
device_batch_size = 32          # ç”¨æˆ·æŒ‡å®šï¼šæ¯ GPU æ‰¹æ¬¡å¤§å°
total_batch_size = 524288       # ç”¨æˆ·æŒ‡å®šï¼šæ€»æ‰¹æ¬¡å¤§å°ï¼ˆå›ºå®šï¼‰
max_seq_len = 2048              # ç”¨æˆ·æŒ‡å®šï¼šåºåˆ—é•¿åº¦ï¼ˆå›ºå®šï¼‰
target_param_data_ratio = 20    # ç”¨æˆ·æŒ‡å®šï¼šæ•°æ®å‚æ•°æ¯”ï¼ˆé€šå¸¸å›ºå®šï¼‰

# ============ é˜¶æ®µ 2ï¼šæ¨¡å‹æ¶æ„è®¡ç®— ============
# æ ¹æ® depth è‡ªåŠ¨æ¨å¯¼æ¨¡å‹æ¶æ„
model_dim = depth * 64
# depth=20 -> model_dim=1280

num_heads = max(1, (model_dim + 127) // 128)
# model_dim=1280 -> num_heads=10 (å‘ä¸Šå–æ•´ï¼Œä¿è¯ head_dim=128)

num_kv_heads = num_heads  # MQA/GQA æ¯”ä¾‹ï¼ˆé»˜è®¤ 1:1ï¼Œå³æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼‰

# ============ é˜¶æ®µ 3ï¼šå®ä¾‹åŒ–æ¨¡å‹å¹¶ç»Ÿè®¡å‚æ•° ============
model = GPT(GPTConfig(
    sequence_len=max_seq_len,
    vocab_size=vocab_size,  # ä»åˆ†è¯å™¨è·å–ï¼Œé€šå¸¸æ˜¯ 65536
    n_layer=depth,
    n_head=num_heads,
    n_kv_head=num_kv_heads,
    n_embd=model_dim,
))

num_params = sum(p.numel() for p in model.parameters())
# depth=20 -> num_params â‰ˆ 561M

# ============ é˜¶æ®µ 4ï¼šè®¡ç®—æ‰¹æ¬¡å¤„ç†å‚æ•° ============
# ç¯å¢ƒä¿¡æ¯ï¼ˆè‡ªåŠ¨æ£€æµ‹æˆ–ä» torchrun è·å–ï¼‰
ddp_world_size = 8  # GPU æ•°é‡

# å•ä¸ª GPU çš„å•æ¬¡å‰å‘ä¼ æ’­å¤„ç†çš„ token æ•°
tokens_per_fwdbwd = device_batch_size * max_seq_len
# 32 * 2048 = 65,536 tokens

# æ‰€æœ‰ GPU çš„å•æ¬¡å‰å‘ä¼ æ’­å¤„ç†çš„ token æ•°ï¼ˆä¸è€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size
# 65,536 * 8 = 524,288 tokens

# è®¡ç®—éœ€è¦çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd
# 524,288 // 524,288 = 1 æ­¥ï¼ˆä¸éœ€è¦æ¢¯åº¦ç´¯ç§¯ï¼‰

# ============ é˜¶æ®µ 5ï¼šè®¡ç®—è®­ç»ƒæŒç»­æ—¶é—´ ============
# æ–¹æ¡ˆ Aï¼šä½¿ç”¨ Chinchilla æ¯”ä¾‹ï¼ˆé»˜è®¤ï¼‰
if target_param_data_ratio > 0:
    target_tokens = target_param_data_ratio * num_params
    # 20 * 561,000,000 = 11,220,000,000 tokens
    
    num_iterations = target_tokens // total_batch_size
    # 11,220,000,000 // 524,288 = 21,408 iterations

# æ–¹æ¡ˆ Bï¼šç›´æ¥æŒ‡å®šè®­ç»ƒæ­¥æ•°
elif num_iterations > 0:
    # ç”¨æˆ·ç›´æ¥æŒ‡å®šï¼Œä¾‹å¦‚ --num_iterations=5000
    pass

# æ–¹æ¡ˆ Cï¼šæŒ‡å®šç›®æ ‡ FLOPs
elif target_flops > 0:
    num_flops_per_token = model.estimate_flops()
    num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))

# è®¡ç®—æ€»è®­ç»ƒ token æ•°
total_tokens = total_batch_size * num_iterations
# 524,288 * 21,408 = 11,220,000,000 tokens

# ============ é˜¶æ®µ 6ï¼šè®¡ç®—æ‰€éœ€æ•°æ®é›†å¤§å° ============
# åˆ†è¯å™¨çš„å‹ç¼©ç‡ï¼ˆé€šè¿‡ tok_eval è·å¾—ï¼‰
chars_per_token = 4.8  # å¤§çº¦å€¼

# æ€»å­—ç¬¦æ•°
total_chars = total_tokens * chars_per_token
# 11,220,000,000 * 4.8 = 53,856,000,000 chars

# æ¯ä¸ªæ•°æ®åˆ†ç‰‡çš„å­—ç¬¦æ•°ï¼ˆFineWeb-Edu æ•°æ®é›†ï¼‰
chars_per_shard = 250_000_000

# éœ€è¦çš„åˆ†ç‰‡æ•°
shards_needed = total_chars / chars_per_shard
# 53,856,000,000 / 250,000,000 = 215.4 -> å‘ä¸Šå–æ•´åˆ° 216 æˆ– 240ï¼ˆç•™ä½™é‡ï¼‰
```

### 2.2 å‚æ•°ä¾èµ–å›¾

```
ç”¨æˆ·è¾“å…¥ depth (20)
    â”‚
    â”œâ”€â”€â†’ model_dim (1280)
    â”‚       â”‚
    â”‚       â””â”€â”€â†’ num_heads (10)
    â”‚               â”‚
    â”‚               â””â”€â”€â†’ model å®ä¾‹åŒ–
    â”‚                       â”‚
    â”‚                       â””â”€â”€â†’ num_params (561M)
    â”‚                               â”‚
    â”‚                               â””â”€â”€â†’ target_tokens (11.2B) â”€â”
    â”‚                                                            â”‚
ç”¨æˆ·è¾“å…¥ device_batch_size (32)                                  â”‚
    â”‚                                                            â”‚
    â”œâ”€â”€â†’ tokens_per_fwdbwd (65,536)                             â”‚
    â”‚       â”‚                                                    â”‚
    â”‚       â””â”€â”€â†’ world_tokens_per_fwdbwd (524,288)              â”‚
    â”‚               â”‚                                            â”‚
    â”‚               â”œâ”€â”€â†’ grad_accum_steps (1)                   â”‚
    â”‚               â”‚                                            â”‚
    â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚                                                       â”‚    â”‚
å›ºå®š total_batch_size (524,288) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
                                                           â”‚    â”‚
                                                           â–¼    â–¼
                                                     num_iterations
                                                           â”‚
                                                           â–¼
                                                      total_tokens
                                                           â”‚
                                                           â–¼
                                                      shards_needed
```

---

## ä¸‰ã€æ•°æ®é›†ç›¸å…³å‚æ•°

### 3.1 æ•°æ®é›†å¤§å°ä¸è®­ç»ƒæ­¥æ•°çš„å…³ç³»

**å…³é”®å…¬å¼**ï¼š

```python
# 1. è®­ç»ƒæ­¥æ•° â†’ æ€» token æ•°
total_tokens = num_iterations Ã— total_batch_size

# 2. æ€» token æ•° â†’ æ€»å­—ç¬¦æ•°ï¼ˆå–å†³äºåˆ†è¯å™¨å‹ç¼©ç‡ï¼‰
total_chars = total_tokens Ã— chars_per_token

# 3. æ€»å­—ç¬¦æ•° â†’ æ‰€éœ€åˆ†ç‰‡æ•°
shards_needed = total_chars / chars_per_shard

# 4. åå‘ï¼šåˆ†ç‰‡æ•° â†’ å¯æ”¯æŒçš„è®­ç»ƒæ­¥æ•°
max_iterations = (shards Ã— chars_per_shard / chars_per_token) / total_batch_size
```

### 3.2 æ•°æ®é›†åˆ‡ç‰‡æ•°é‡çš„å½±å“

**æƒ…å†µ 1ï¼šæ•°æ®å……è¶³ï¼ˆåˆ†ç‰‡ â‰¥ éœ€æ±‚ï¼‰**

```python
# å‡è®¾ä¸‹è½½äº† 240 ä¸ªåˆ†ç‰‡
available_shards = 240
available_chars = 240 * 250_000_000 = 60,000,000,000 chars
available_tokens = 60,000,000,000 / 4.8 = 12,500,000,000 tokens

# å¯æ”¯æŒçš„è®­ç»ƒæ­¥æ•°
max_iterations = 12,500,000,000 / 524,288 = 23,841 iterations

# å¦‚æœå®é™…åªè®­ç»ƒ 21,408 iterationsï¼ˆæ ¹æ® Chinchillaï¼‰
# åˆ™æ•°æ®è¶³å¤Ÿï¼Œæ¯ä¸ªæ ·æœ¬åªçœ‹ä¸€æ¬¡ï¼ˆ1 epochï¼‰
```

**æƒ…å†µ 2ï¼šæ•°æ®ä¸è¶³ï¼ˆåˆ†ç‰‡ < éœ€æ±‚ï¼‰**

```python
# å‡è®¾åªä¸‹è½½äº† 100 ä¸ªåˆ†ç‰‡
available_shards = 100
available_chars = 100 * 250_000_000 = 25,000,000,000 chars
available_tokens = 25,000,000,000 / 4.8 = 5,208,333,333 tokens

# å¯æ”¯æŒçš„è®­ç»ƒæ­¥æ•°ï¼ˆ1 epochï¼‰
max_iterations_1epoch = 5,208,333,333 / 524,288 = 9,933 iterations

# ä½†å®é™…éœ€è¦è®­ç»ƒ 21,408 iterations
# ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
# â†’ æ•°æ®åŠ è½½å™¨ä¼šå¾ªç¯è¯»å–ï¼Œè¿›è¡Œå¤šä¸ª epoch
num_epochs = 21,408 / 9,933 = 2.16 epochs

# å½±å“ï¼š
# - è®­ç»ƒä»ç„¶å¯ä»¥è¿›è¡Œ
# - ä½†è¿‡æ‹Ÿåˆé£é™©å¢åŠ ï¼ˆæ•°æ®é‡å¤ï¼‰
# - å­¦ä¹ æ•ˆç‡å¯èƒ½é™ä½
```

**é‡è¦ï¼šæ•°æ®åŠ è½½å™¨çš„å¾ªç¯æœºåˆ¶**

```python
# nanochat/dataloader.py

def tokenizing_distributed_data_loader(B, T, split, ...):
    def document_batches():
        while True:  # â† æ— é™å¾ªç¯ï¼
            for batch in parquets_iter_batched(split=split, ...):
                yield batch
    
    # ç‰¹ç‚¹ï¼š
    # 1. æ•°æ®æ°¸è¿œä¸ä¼š"ç”¨å®Œ"
    # 2. è¯»å®Œæ‰€æœ‰åˆ†ç‰‡åï¼Œè‡ªåŠ¨ä»å¤´å¼€å§‹
    # 3. è®­ç»ƒæ­¥æ•°å®Œå…¨ç”± num_iterations æ§åˆ¶
    # 4. æ•°æ®é›†å¤§å°åªå½±å“æ˜¯å¦é‡å¤ï¼ˆepoch æ•°ï¼‰
```

### 3.3 æ•°æ®é›†å¤§å°çš„å®é™…è®¡ç®—ï¼ˆspeedrun.shï¼‰

```bash
# æ­¥éª¤ 1ï¼šè®¡ç®—æ¨¡å‹å‚æ•°æ•°
# d20 æ¨¡å‹ -> 561M å‚æ•°

# æ­¥éª¤ 2ï¼šåº”ç”¨ Chinchilla æ¯”ä¾‹
# tokens_needed = 20 Ã— 561M = 11.2B tokens

# æ­¥éª¤ 3ï¼šè€ƒè™‘åˆ†è¯å™¨å‹ç¼©ç‡
# chars_needed = 11.2B Ã— 4.8 = 53.8B chars

# æ­¥éª¤ 4ï¼šè®¡ç®—åˆ†ç‰‡æ•°
# shards_needed = 53.8B / 250M = 215.4 shards

# æ­¥éª¤ 5ï¼šå‘ä¸Šå–æ•´å¹¶ç•™ä½™é‡
# download 240 shards  # æ³¨é‡Šè¯´æ˜ï¼š240 æ˜¯ç²¾å¿ƒè®¡ç®—çš„

# éªŒè¯ï¼š
# 240 shards Ã— 250M chars/shard = 60B chars
# 60B chars / 4.8 = 12.5B tokensï¼ˆç•¥å¤šäº 11.2Bï¼Œç•™ 11% ä½™é‡ï¼‰
```

---

## å››ã€æ‰¹æ¬¡å¤§å°å‚æ•°è¯¦è§£

### 4.1 ä¸ºä»€ä¹ˆ total_batch_size = 524,288ï¼Ÿ

**1. ç»éªŒå€¼ï¼ˆScaling Lawsï¼‰**

```python
# æ ¹æ® GPT ç³»åˆ—å’Œ Chinchilla è®ºæ–‡çš„ç»éªŒï¼š
# - å°æ¨¡å‹ï¼ˆ<1Bï¼‰ï¼šæ¨è 512K - 1M tokens/batch
# - ä¸­æ¨¡å‹ï¼ˆ1-10Bï¼‰ï¼šæ¨è 1M - 4M tokens/batch
# - å¤§æ¨¡å‹ï¼ˆ>10Bï¼‰ï¼šæ¨è 4M+ tokens/batch

# nanochat çš„ d20 æ¨¡å‹ï¼ˆ561M å‚æ•°ï¼‰ï¼š
total_batch_size = 524,288  # = 512Kï¼Œåœ¨æ¨èèŒƒå›´å†…
```

**2. è®¡ç®—æ•ˆç‡è€ƒè™‘**

```python
# ç›®æ ‡ï¼šå……åˆ†åˆ©ç”¨ 8XH100 GPU
# æ¯ä¸ª H100ï¼š
#   - 80GB VRAM
#   - 989 TFLOPSï¼ˆBF16ï¼‰

# åœ¨ device_batch_size=32, seq_len=2048 æ—¶ï¼š
# - å• GPU å•æ­¥ï¼š65,536 tokens
# - 8 GPU å•æ­¥ï¼š524,288 tokens
# - æ¢¯åº¦ç´¯ç§¯ï¼š1 æ­¥ï¼ˆæ— éœ€ç´¯ç§¯ï¼‰
# 
# è¿™æ„å‘³ç€ï¼šæ¯æ¬¡å‰å‘+åå‘ä¼ æ’­åç«‹å³æ›´æ–°å‚æ•°
# â†’ è®¡ç®—æ•ˆç‡é«˜ï¼Œé€šä¿¡å¼€é”€ä½
```

**3. ç¨³å®šæ€§è€ƒè™‘**

```python
# æ‰¹æ¬¡å¤§å°å½±å“è®­ç»ƒç¨³å®šæ€§ï¼š
# - å¤ªå°ï¼šæ¢¯åº¦å™ªå£°å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š
# - å¤ªå¤§ï¼šæ¢¯åº¦è¿‡äºå¹³æ»‘ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
# - 512Kï¼šç»éªŒè¯çš„ç¨³å®šå€¼
```

### 4.2 total_batch_size æ˜¯å¦å¯ä»¥æ”¹å˜ï¼Ÿ

**å¯ä»¥æ”¹å˜ï¼Œä½†éœ€è¦ç›¸åº”è°ƒæ•´**ï¼š

```python
# åœºæ™¯ 1ï¼šæ˜¾å­˜ä¸è¶³ï¼Œå‡å° device_batch_size
device_batch_size = 16  # ä» 32 å‡åŠ
# åæœï¼š
world_tokens_per_fwdbwd = 16 * 2048 * 8 = 262,144 tokens
grad_accum_steps = 524,288 / 262,144 = 2 æ­¥
# å½±å“ï¼šéœ€è¦ 2 æ­¥æ¢¯åº¦ç´¯ç§¯ï¼Œè®­ç»ƒé€Ÿåº¦ç•¥æ…¢ï¼ˆä½†ä»å¯è¡Œï¼‰

# åœºæ™¯ 2ï¼šåªæœ‰å• GPU
ddp_world_size = 1
device_batch_size = 32
world_tokens_per_fwdbwd = 32 * 2048 * 1 = 65,536 tokens
grad_accum_steps = 524,288 / 65,536 = 8 æ­¥
# å½±å“ï¼šéœ€è¦ 8 æ­¥æ¢¯åº¦ç´¯ç§¯ï¼Œè®­ç»ƒé€Ÿåº¦æ…¢ 8 å€

# åœºæ™¯ 3ï¼šæ”¹å˜ total_batch_size
total_batch_size = 1_048_576  # ç¿»å€
# åæœï¼š
# - éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¯èƒ½éœ€è¦å¢å¤§ï¼‰
# - è®­ç»ƒæ›´ç¨³å®šï¼Œä½†æ”¶æ•›å¯èƒ½æ›´æ…¢
# - å¦‚æœç¡¬ä»¶ä¸å˜ï¼Œéœ€è¦æ›´å¤šæ¢¯åº¦ç´¯ç§¯
grad_accum_steps = 1_048_576 / 524,288 = 2 æ­¥
```

### 4.3 æ¢¯åº¦ç´¯ç§¯çš„æœ¬è´¨

```python
# ç­‰ä»·å…³ç³»ï¼š
# 
# é…ç½® Aï¼š8 GPU, batch=32, grad_accum=1
# = é…ç½® Bï¼š4 GPU, batch=32, grad_accum=2
# = é…ç½® Cï¼š1 GPU, batch=32, grad_accum=8
# = é…ç½® Dï¼š8 GPU, batch=16, grad_accum=2
# 
# éƒ½ä¼šäº§ç”Ÿç›¸åŒçš„ï¼š
# - æ¢¯åº¦ï¼ˆæœŸæœ›ç›¸åŒï¼‰
# - å‚æ•°æ›´æ–°
# - æœ€ç»ˆæ¨¡å‹
# 
# åŒºåˆ«ï¼š
# - æ—¶é—´ï¼šé…ç½® A æœ€å¿«ï¼Œé…ç½® C æœ€æ…¢
# - å†…å­˜ï¼šbatch=16 æ¯” batch=32 çœå†…å­˜
```

**å®ç°ç»†èŠ‚**ï¼š

```python
# scripts/base_train.py

for micro_step in range(grad_accum_steps):
    with autocast_ctx:
        loss = model(x, y)
    
    # å…³é”®ï¼šæŸå¤±å½’ä¸€åŒ–
    loss = loss / grad_accum_steps  # â† æ¯æ¬¡ç´¯ç§¯çš„æ¢¯åº¦æ˜¯æ€»æ¢¯åº¦çš„ 1/N
    loss.backward()  # ç´¯ç§¯æ¢¯åº¦
    
    x, y = next(train_loader)  # é¢„å–ä¸‹ä¸€æ‰¹æ•°æ®

# æ‰€æœ‰ micro-batch ç´¯ç§¯å®Œæˆåï¼Œæ›´æ–°å‚æ•°
optimizer.step()
model.zero_grad()
```

---

## äº”ã€ä¸åŒè®­ç»ƒé˜¶æ®µçš„å‚æ•°ç­–ç•¥

### 5.1 é¢„è®­ç»ƒï¼ˆBase Trainingï¼‰

```python
# ç›®æ ‡ï¼šåœ¨å¤§è§„æ¨¡æ–‡æœ¬ä¸Šå­¦ä¹ è¯­è¨€å»ºæ¨¡
# ç­–ç•¥ï¼šæ•°æ®é‡ >> å‚æ•°é‡ï¼ˆChinchilla: 20Ã—ï¼‰

depth = 20
total_batch_size = 524_288      # å›ºå®š
target_param_data_ratio = 20    # å›ºå®š
max_seq_len = 2048              # å›ºå®š

# è‡ªåŠ¨è®¡ç®—ï¼š
num_params = 561_000_000
num_iterations = 21_408
total_tokens = 11_220_000_000
shards_needed = 240

# æ•°æ®é›†ï¼šFineWeb-Eduï¼ˆé€šç”¨æ–‡æœ¬ï¼‰
# - æ— é™å¾ªç¯è¯»å–
# - ç†è®ºä¸Šéœ€è¦ 240 ä¸ªåˆ†ç‰‡ï¼ˆ1 epochï¼‰
# - å®é™…å¯ä»¥å°‘ä¸€äº›ï¼ˆå…è®¸å°‘é‡é‡å¤ï¼‰
```

### 5.2 ä¸­é—´è®­ç»ƒï¼ˆMidtrainingï¼‰

```python
# ç›®æ ‡ï¼šå­¦ä¹ å¯¹è¯æ ¼å¼å’Œå·¥å…·ä½¿ç”¨
# ç­–ç•¥ï¼šå°‘é‡æ•°æ®ï¼Œå¤š epoch

# å‚æ•°é€šå¸¸ç»§æ‰¿è‡ª base_train.pyï¼š
depth = 20  # ç»§æ‰¿
device_batch_size = 32  # å¯èƒ½ç›¸åŒ
total_batch_size = 524_288  # é€šå¸¸ç›¸åŒ

# ä½†è®­ç»ƒæ—¶é—´æ›´çŸ­ï¼š
num_iterations = 2000  # ç¤ºä¾‹ï¼Œè¿œå°‘äºé¢„è®­ç»ƒ
# æˆ–è€… num_epochs = 1-3

# æ•°æ®é›†ï¼š
# - SmolTalk + ä»»åŠ¡æ•°æ® + åˆæˆæ•°æ®
# - æ€»é‡ï¼š~50K - 100K æ ·æœ¬
# - ç‰¹ç‚¹ï¼šé«˜è´¨é‡ã€æ ¼å¼åŒ–
# - å…è®¸å¤š epochï¼ˆ3-5 epoch å¸¸è§ï¼‰

# æ‰¹æ¬¡å¤§å°å¯èƒ½ä¸å˜ï¼Œä½†å› æ•°æ®å°‘ï¼š
# - å®é™… epoch æ•°æ›´å¤š
# - æˆ–å‡å°‘ num_iterations
```

### 5.3 ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

```python
# ç›®æ ‡ï¼šåœ¨é«˜è´¨é‡ä»»åŠ¡æ•°æ®ä¸Šç²¾è°ƒ
# ç­–ç•¥ï¼šå°æ‰¹æ¬¡ã€å°‘é‡æ•°æ®ã€å¿«é€Ÿæ”¶æ•›

# æ‰¹æ¬¡å¤§å°é€šå¸¸å‡å°ï¼š
device_batch_size = 4  # ä» 32 å‡å°åˆ° 4
target_examples_per_step = 32  # ç›®æ ‡æ ·æœ¬æ•°
total_batch_size = ä¸å†æ˜¯ tokensï¼Œè€Œæ˜¯ examples

# ä¸ºä»€ä¹ˆå‡å°ï¼Ÿ
# - SFT æ•°æ®é•¿åº¦å˜åŒ–å¤§ï¼ˆ100 - 2048 tokensï¼‰
# - ä½¿ç”¨åŠ¨æ€ padding
# - é¿å…æµªè´¹è®¡ç®—ï¼ˆpadding å¤ªå¤šï¼‰

# æ•°æ®é›†ï¼š
train_ds = TaskMixture([
    ARC_Easy,       # 2.3K
    ARC_Challenge,  # 1.1K
    GSM8K,          # 8K
    SmolTalk,       # 10K
    # ...
])  # æ€»è®¡ ~23K æ ·æœ¬

# è®­ç»ƒï¼š
num_epochs = 1  # é€šå¸¸ 1 epoch è¶³å¤Ÿ
num_iterations = 23_000 / 32 = 718 stepsï¼ˆå¦‚æœ batch=32ï¼‰

# æ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚æœæœ‰ 8 GPUï¼‰ï¼š
examples_per_step = 4 * 8 = 32
grad_accum_steps = 32 / 32 = 1ï¼ˆä¸éœ€è¦ç´¯ç§¯ï¼‰
```

### 5.4 å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰

```python
# ç›®æ ‡ï¼šé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç‰¹å®šä»»åŠ¡
# ç­–ç•¥ï¼šåœ¨çº¿é‡‡æ ·ã€å°æ‰¹æ¬¡ã€å¿«é€Ÿè¿­ä»£

# ç‰¹æ®Šçš„æ‰¹æ¬¡æ¦‚å¿µï¼š
num_samples_per_prompt = 4  # æ¯ä¸ªé—®é¢˜é‡‡æ · 4 ä¸ªç­”æ¡ˆ
num_prompts_per_batch = 8   # æ¯æ‰¹ 8 ä¸ªé—®é¢˜
effective_batch = 4 * 8 = 32 ä¸ªæ ·æœ¬

# æ•°æ®é›†ï¼š
# - GSM8K train splitï¼ˆ8K é—®é¢˜ï¼‰
# - ä½†æ¯ä¸ª iteration åªç”¨ä¸€å°éƒ¨åˆ†ï¼ˆä¾‹å¦‚ 32 ä¸ªï¼‰
# - åœ¨çº¿é‡‡æ ·ï¼šæ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆç­”æ¡ˆ

# è®­ç»ƒæ—¶é—´ï¼š
num_iterations = 500 - 2000  # é€šå¸¸è¾ƒå°‘
# ä½†æ¯ä¸ª iteration å¾ˆæ…¢ï¼ˆéœ€è¦æ¨ç†ç”Ÿæˆç­”æ¡ˆï¼‰

# æ³¨æ„ï¼šRL ä¸ä½¿ç”¨ä¼ ç»Ÿçš„ total_batch_size
# è€Œæ˜¯ç”±é‡‡æ ·ç­–ç•¥æ§åˆ¶
```

---

## å…­ã€å®é™…è®¡ç®—ç¤ºä¾‹

### 6.1 ç¤ºä¾‹ 1ï¼šæ ‡å‡† d20 æ¨¡å‹ï¼ˆspeedrun.shï¼‰

```python
# ============ è¾“å…¥ ============
depth = 20
device_batch_size = 32
total_batch_size = 524_288
max_seq_len = 2048
ddp_world_size = 8
target_param_data_ratio = 20
vocab_size = 65_536

# ============ è®¡ç®—æ¨¡å‹æ¶æ„ ============
model_dim = 20 * 64 = 1280
num_heads = (1280 + 127) // 128 = 11 headsï¼ˆå®é™…ä»£ç ç”¨ max(1, ...)ï¼‰
# ç»è¿‡å®ä¾‹åŒ–åç»Ÿè®¡ï¼š
num_params = 561_000_000

# ============ è®¡ç®—æ‰¹æ¬¡å‚æ•° ============
tokens_per_fwdbwd = 32 * 2048 = 65_536 tokens
world_tokens_per_fwdbwd = 65_536 * 8 = 524_288 tokens
grad_accum_steps = 524_288 / 524_288 = 1 æ­¥ï¼ˆæ— éœ€ç´¯ç§¯ï¼‰

# ============ è®¡ç®—è®­ç»ƒæŒç»­æ—¶é—´ ============
target_tokens = 20 * 561_000_000 = 11_220_000_000 tokens
num_iterations = 11_220_000_000 / 524_288 = 21_408 steps

# ============ è®¡ç®—æ•°æ®éœ€æ±‚ ============
chars_per_token = 4.8  # ä» tok_eval è·å¾—
total_chars = 11_220_000_000 * 4.8 = 53_856_000_000 chars
shards_needed = 53_856_000_000 / 250_000_000 = 215.4 shards
# å‘ä¸Šå–æ•´å¹¶ç•™ä½™é‡ â†’ ä¸‹è½½ 240 shards

# ============ è®­ç»ƒæ—¶é—´ä¼°ç®— ============
# å‡è®¾ MFU = 40%ï¼ˆå®é™…æµ‹å¾—ï¼‰
promised_flops = 989e12 * 8 = 7.912e15 FLOPS
actual_flops = 7.912e15 * 0.40 = 3.165e15 FLOPS

num_flops_per_token = 6 * num_params  # ç®€åŒ–ä¼°ç®—
total_flops = 6 * 561e6 * 11.22e9 = 3.78e19 FLOPS

training_time = 3.78e19 / 3.165e15 = 11_943 seconds = 3.3 hours

# åŠ ä¸Šæ•°æ®åŠ è½½ã€è¯„ä¼°ç­‰å¼€é”€ â†’ çº¦ 4 å°æ—¶ï¼ˆä¸å®é™…ç›¸ç¬¦ï¼ï¼‰
```

### 6.2 ç¤ºä¾‹ 2ï¼šæ›´å¤§çš„ d26 æ¨¡å‹

```python
# ============ è¾“å…¥è°ƒæ•´ ============
depth = 26
device_batch_size = 16  # â† å‡åŠï¼Œé¿å… OOM
total_batch_size = 524_288  # â† ä¿æŒä¸å˜
# å…¶ä»–å‚æ•°åŒ d20

# ============ è®¡ç®—æ¨¡å‹æ¶æ„ ============
model_dim = 26 * 64 = 1664
num_heads = (1664 + 127) // 128 = 14 heads
num_params = 1_200_000_000  # çº¦ 1.2B

# ============ è®¡ç®—æ‰¹æ¬¡å‚æ•° ============
tokens_per_fwdbwd = 16 * 2048 = 32_768 tokensï¼ˆå‡åŠï¼‰
world_tokens_per_fwdbwd = 32_768 * 8 = 262_144 tokens
grad_accum_steps = 524_288 / 262_144 = 2 æ­¥ï¼ˆéœ€è¦ç´¯ç§¯ï¼ï¼‰

# å½±å“ï¼š
# - æ¯ä¸ªè®­ç»ƒæ­¥éœ€è¦ 2 æ¬¡å‰å‘+åå‘ä¼ æ’­
# - è®­ç»ƒé€Ÿåº¦çº¦æ…¢ 2 å€
# - ä½†å‚æ•°æ›´æ–°ä»ç„¶ä½¿ç”¨ç›¸åŒçš„æ€»æ‰¹æ¬¡å¤§å°

# ============ è®¡ç®—è®­ç»ƒæŒç»­æ—¶é—´ ============
target_tokens = 20 * 1_200_000_000 = 24_000_000_000 tokens
num_iterations = 24_000_000_000 / 524_288 = 45_776 steps

# ============ è®¡ç®—æ•°æ®éœ€æ±‚ ============
total_chars = 24_000_000_000 * 4.8 = 115_200_000_000 chars
shards_needed = 115_200_000_000 / 250_000_000 = 460.8 shards
# å‘ä¸Šå–æ•´ â†’ ä¸‹è½½ 480 shards

# ============ è®­ç»ƒæ—¶é—´ä¼°ç®— ============
# å‚æ•°é‡å¢åŠ  2.14 å€
# è®­ç»ƒæ­¥æ•°å¢åŠ  2.14 å€
# æ¢¯åº¦ç´¯ç§¯å¢åŠ  2 å€
# â†’ æ€»è®­ç»ƒæ—¶é—´çº¦ 4 * 2.14 * 2 = 17 å°æ—¶ï¼ˆå®é™…çº¦ 12 å°æ—¶ï¼Œå›  MFU æå‡ï¼‰
```

### 6.3 ç¤ºä¾‹ 3ï¼šå• GPU è®­ç»ƒå°æ¨¡å‹

```python
# ============ è¾“å…¥è°ƒæ•´ ============
depth = 12
device_batch_size = 8
total_batch_size = 524_288  # â† ä»ç„¶ä¿æŒï¼Œä½†éœ€å¤§é‡ç´¯ç§¯
max_seq_len = 2048
ddp_world_size = 1  # â† å• GPUï¼

# ============ è®¡ç®—æ¨¡å‹æ¶æ„ ============
model_dim = 12 * 64 = 768
num_heads = (768 + 127) // 128 = 7 heads
num_params = 150_000_000  # çº¦ 150M

# ============ è®¡ç®—æ‰¹æ¬¡å‚æ•° ============
tokens_per_fwdbwd = 8 * 2048 = 16_384 tokens
world_tokens_per_fwdbwd = 16_384 * 1 = 16_384 tokens
grad_accum_steps = 524_288 / 16_384 = 32 æ­¥ï¼ˆå¤§é‡ç´¯ç§¯ï¼ï¼‰

# å½±å“ï¼š
# - æ¯ä¸ªå‚æ•°æ›´æ–°éœ€è¦ 32 æ¬¡å‰å‘+åå‘ä¼ æ’­
# - è®­ç»ƒé€Ÿåº¦çº¦æ…¢ 32 å€ï¼ˆç›¸æ¯” 8 GPU æ— ç´¯ç§¯ï¼‰
# - ä½†æœ€ç»ˆæ•ˆæœç›¸åŒï¼ˆå‡è®¾æ•°æ®ç›¸åŒï¼‰

# ============ è®¡ç®—è®­ç»ƒæŒç»­æ—¶é—´ ============
target_tokens = 20 * 150_000_000 = 3_000_000_000 tokens
num_iterations = 3_000_000_000 / 524_288 = 5_722 steps

# ============ è®¡ç®—æ•°æ®éœ€æ±‚ ============
total_chars = 3_000_000_000 * 4.8 = 14_400_000_000 chars
shards_needed = 14_400_000_000 / 250_000_000 = 57.6 shards
# å‘ä¸Šå–æ•´ â†’ ä¸‹è½½ 60 shards

# ============ è®­ç»ƒæ—¶é—´ä¼°ç®— ============
# å• GPU (ä¾‹å¦‚ RTX 4090)ï¼š82 TFLOPS (BF16)
# MFU å‡è®¾ 30%
actual_flops = 82e12 * 0.30 = 24.6e12 FLOPS

num_flops_per_token = 6 * 150e6 = 9e8 FLOPS/token
total_flops = 9e8 * 3e9 = 2.7e18 FLOPS

training_time = 2.7e18 / 24.6e12 = 109_756 seconds = 30.5 hours
```

### 6.4 ç¤ºä¾‹ 4ï¼šå¦‚æœæ•°æ®ä¸è¶³ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

```python
# ============ åœºæ™¯ï¼šåªä¸‹è½½äº† 100 ä¸ªåˆ†ç‰‡ ============
available_shards = 100
available_chars = 100 * 250_000_000 = 25_000_000_000 chars
available_tokens = 25_000_000_000 / 4.8 = 5_208_333_333 tokens

# ============ ä½†æ¨¡å‹éœ€è¦ï¼ˆd20ï¼‰============
required_tokens = 11_220_000_000 tokens
num_iterations = 21_408 steps

# ============ åˆ†æ ============
# æ•°æ®å¯ä»¥æ”¯æŒï¼ˆ1 epochï¼‰ï¼š
max_iterations_1epoch = 5_208_333_333 / 524_288 = 9_933 steps

# å®é™…éœ€è¦è®­ç»ƒï¼š
actual_iterations = 21_408 steps

# æ•°æ®å°†è¢«é‡å¤ï¼š
num_epochs = 21_408 / 9_933 = 2.16 epochs

# ============ å½±å“ ============
# è®­ç»ƒä»ç„¶å¯ä»¥å®Œæˆï¼Œå› ä¸ºæ•°æ®åŠ è½½å™¨ä¼šå¾ªç¯
# ä½†ï¼š
# 1. è¿‡æ‹Ÿåˆé£é™©å¢åŠ ï¼ˆæ¯ä¸ªæ ·æœ¬çœ‹äº† 2.16 æ¬¡ï¼‰
# 2. éªŒè¯æŸå¤±å¯èƒ½ä¸å†å‡†ç¡®ï¼ˆéªŒè¯é›†ä¹Ÿè¢«é‡å¤ï¼‰
# 3. æ³›åŒ–èƒ½åŠ›å¯èƒ½ä¸‹é™
# 4. å­¦ä¹ æ•ˆç‡é™ä½ï¼ˆé‡å¤æ•°æ®å¸¦æ¥çš„ä¿¡æ¯å‡å°‘ï¼‰

# å»ºè®®ï¼š
# - å°½é‡ä¿è¯æ•°æ®å……è¶³ï¼ˆâ‰¥ 1 epochï¼‰
# - æˆ–å‡å°‘ num_iterations
# - æˆ–å‡å° target_param_data_ratioï¼ˆä¾‹å¦‚ 15Ã— è€Œé 20Ã—ï¼‰
```

---

## ä¸ƒã€æ€»ç»“ä¸å»ºè®®

### 7.1 æ ¸å¿ƒè¦ç‚¹

1. **total_batch_size é€šå¸¸å›ºå®šä¸º 524,288**ï¼š
   - è¿™æ˜¯ç»éªŒå€¼ï¼Œé€‚åˆ <1B å‚æ•°çš„æ¨¡å‹
   - å¯ä»¥æ”¹å˜ï¼Œä½†éœ€è¦ç›¸åº”è°ƒæ•´å­¦ä¹ ç‡å’Œè®­ç»ƒæ­¥æ•°

2. **è®­ç»ƒæ­¥æ•°ç”± Chinchilla æ¯”ä¾‹è‡ªåŠ¨è®¡ç®—**ï¼š
   - `num_iterations = (20 Ã— num_params) / total_batch_size`
   - å¯ä»¥æ‰‹åŠ¨æŒ‡å®š `--num_iterations` è¦†ç›–

3. **æ•°æ®é›†å¤§å°ç”±è®­ç»ƒæ­¥æ•°å†³å®š**ï¼š
   - `shards_needed = (num_iterations Ã— total_batch_size Ã— chars_per_token) / chars_per_shard`
   - æ•°æ®ä¸è¶³æ—¶ä¼šå¾ªç¯ï¼Œå…è®¸å¤š epochï¼ˆä½†ä¸æ¨èï¼‰

4. **æ¢¯åº¦ç´¯ç§¯è‡ªåŠ¨è¡¥å¿ç¡¬ä»¶é™åˆ¶**ï¼š
   - å• GPUã€å°æ˜¾å­˜ã€å°‘ GPU éƒ½å¯ä»¥è®­ç»ƒ
   - åªæ˜¯é€Ÿåº¦å˜æ…¢ï¼Œæœ€ç»ˆæ•ˆæœç›¸åŒ

5. **ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒç­–ç•¥**ï¼š
   - é¢„è®­ç»ƒï¼šå¤§æ‰¹æ¬¡ã€å¤§æ•°æ®ã€1 epoch
   - Midtrainingï¼šä¸­æ‰¹æ¬¡ã€ä¸­æ•°æ®ã€2-3 epoch
   - SFTï¼šå°æ‰¹æ¬¡ã€å°æ•°æ®ã€1 epoch
   - RLï¼šç‰¹æ®Šé‡‡æ ·ã€åœ¨çº¿ç”Ÿæˆ

### 7.2 å®ç”¨å»ºè®®

**è°ƒæ•´æ¨¡å‹å¤§å°æ—¶**ï¼š

```bash
# å¦‚æœå¢å¤§æ¨¡å‹ï¼ˆå¢åŠ  depthï¼‰ï¼š
# 1. å¯èƒ½éœ€è¦å‡å° device_batch_sizeï¼ˆé¿å… OOMï¼‰
# 2. è‡ªåŠ¨å¢åŠ  num_iterationsï¼ˆChinchillaï¼‰
# 3. éœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®åˆ†ç‰‡
# 4. è®­ç»ƒæ—¶é—´æ˜¾è‘—å¢åŠ 

# ä¾‹å¦‚ï¼š
# d20 -> d26: æ—¶é—´ Ã— 3, æ•°æ® Ã— 2.14, æˆæœ¬ Ã— 3
```

**è°ƒæ•´ç¡¬ä»¶æ—¶**ï¼š

```bash
# å¦‚æœå‡å°‘ GPU æ•°é‡ï¼š
# 1. grad_accum_steps è‡ªåŠ¨å¢åŠ 
# 2. è®­ç»ƒæ—¶é—´çº¿æ€§å¢åŠ 
# 3. ä¸éœ€è¦æ”¹å˜å…¶ä»–å‚æ•°

# ä¾‹å¦‚ï¼š
# 8 GPU -> 1 GPU: æ—¶é—´ Ã— 8, å…¶ä»–ä¸å˜
```

**è°ƒæ•´æ•°æ®é›†æ—¶**ï¼š

```bash
# å¦‚æœæ•°æ®ä¸è¶³ï¼š
# æ–¹æ¡ˆ Aï¼šä¸‹è½½æ›´å¤šæ•°æ®ï¼ˆæ¨èï¼‰
python -m nanochat.dataset -n 300  # å¢åŠ åˆ° 300 åˆ†ç‰‡

# æ–¹æ¡ˆ Bï¼šå‡å°‘è®­ç»ƒæ­¥æ•°
torchrun ... --num_iterations=10000  # æ‰‹åŠ¨æŒ‡å®š

# æ–¹æ¡ˆ Cï¼šé™ä½ Chinchilla æ¯”ä¾‹ï¼ˆä¸æ¨èï¼‰
# ä¿®æ”¹ä»£ç ï¼štarget_param_data_ratio = 15  # ä» 20 æ”¹ä¸º 15
```

**è°ƒæ•´æ‰¹æ¬¡å¤§å°æ—¶**ï¼š

```bash
# å¦‚æœæ”¹å˜ total_batch_sizeï¼š
# éœ€è¦åŒæ­¥è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæ ¹æ®ç»éªŒï¼‰ï¼š
# lr_new = lr_old Ã— sqrt(batch_new / batch_old)

# ä¾‹å¦‚ï¼š
# batch 524K -> 1M
# lr 0.02 -> 0.02 Ã— sqrt(2) = 0.028
```

### 7.3 å¸¸è§è¯¯åŒº

âŒ **è¯¯åŒº 1**ï¼š"æ•°æ®åˆ†ç‰‡æ•°å†³å®šè®­ç»ƒæ­¥æ•°"
- âœ… æ­£ç¡®ï¼šè®­ç»ƒæ­¥æ•°ç”± `num_iterations` å†³å®šï¼ˆæ ¹æ®æ¨¡å‹å¤§å°è‡ªåŠ¨è®¡ç®—æˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰
- æ•°æ®åˆ†ç‰‡æ•°åªå½±å“æ˜¯å¦éœ€è¦å¤š epoch

âŒ **è¯¯åŒº 2**ï¼š"total_batch_size å¿…é¡»æ˜¯ 524,288"
- âœ… æ­£ç¡®ï¼šè¿™æ˜¯æ¨èå€¼ï¼Œå¯ä»¥æ”¹å˜
- ä½†æ”¹å˜åéœ€è¦è°ƒæ•´å­¦ä¹ ç‡å’Œå¯èƒ½çš„è®­ç»ƒæ­¥æ•°

âŒ **è¯¯åŒº 3**ï¼š"å• GPU æ— æ³•è®­ç»ƒå¤§æ¨¡å‹"
- âœ… æ­£ç¡®ï¼šå¯ä»¥è®­ç»ƒï¼Œé€šè¿‡æ¢¯åº¦ç´¯ç§¯
- åªæ˜¯é€Ÿåº¦æ…¢ï¼Œæ˜¾å­˜ä»ç„¶æ˜¯é™åˆ¶å› ç´ 

âŒ **è¯¯åŒº 4**ï¼š"æ•°æ®ä¸è¶³å°±æ— æ³•è®­ç»ƒ"
- âœ… æ­£ç¡®ï¼šå¯ä»¥è®­ç»ƒï¼Œä½†ä¼šå¤š epoch
- å½±å“æ³›åŒ–èƒ½åŠ›ï¼Œåº”å°½é‡é¿å…

---

## é™„å½•ï¼šå‚æ•°è®¡ç®—å…¬å¼æ±‡æ€»

```python
# ============ æ¨¡å‹æ¶æ„ ============
model_dim = depth Ã— 64
num_heads = ceil(model_dim / 128)
num_params = f(model_dim, depth, vocab_size)  # å¤æ‚å…¬å¼ï¼Œé€šè¿‡å®ä¾‹åŒ–ç»Ÿè®¡

# ============ æ‰¹æ¬¡å¤„ç† ============
tokens_per_fwdbwd = device_batch_size Ã— max_seq_len
world_tokens_per_fwdbwd = tokens_per_fwdbwd Ã— ddp_world_size
grad_accum_steps = total_batch_size / world_tokens_per_fwdbwd

# ============ è®­ç»ƒè§„æ¨¡ ============
target_tokens = target_param_data_ratio Ã— num_params
num_iterations = target_tokens / total_batch_size
total_tokens = total_batch_size Ã— num_iterations

# ============ æ•°æ®éœ€æ±‚ ============
total_chars = total_tokens Ã— chars_per_token
shards_needed = total_chars / chars_per_shard
num_epochs = shards_downloaded / shards_needed  # å¦‚æœ < 1ï¼Œåˆ™å¤š epoch

# ============ æ—¶é—´ä¼°ç®— ============
num_flops_per_token = 6 Ã— (num_params - embedding_params) + 12 Ã— L Ã— H Ã— Q Ã— T
total_flops = num_flops_per_token Ã— total_tokens
training_time = total_flops / (gpu_flops Ã— ddp_world_size Ã— MFU)

# ============ æˆæœ¬ä¼°ç®— ============
total_cost = (training_time / 3600) Ã— gpu_hourly_cost Ã— ddp_world_size
```

---

**æ–‡æ¡£ç»“æŸ**

å¸Œæœ›è¿™ä»½æ–‡æ¡£èƒ½å¸®åŠ©ä½ ç†è§£ nanochat ä¸­å‚æ•°è®¡ç®—çš„æ¥é¾™å»è„‰ï¼ğŸ¯

