# nanochat 中文技术文档

> 这是 nanochat 项目的完整中文技术文档集合，涵盖从入门到进阶的所有内容。

---

## 📚 文档导航

### 🎯 核心文档

#### 1. [nanochat技术文档.md](./nanochat技术文档.md) ⭐⭐⭐⭐⭐
**保姆级完整技术文档（1860 行）**

适合人群：所有用户

内容包括：
- ✅ 项目概述和特点
- ✅ 核心架构详解
- ✅ 模块功能详解（GPT、Engine、Tokenizer、DataLoader 等）
- ✅ 训练流程详解（Base/Mid/SFT/RL）
- ✅ 使用指南（快速开始、自定义训练、评估调试、部署）
- ✅ 总结与最佳实践
- ✅ 附录（命令速查、参数速查、性能基准）

**推荐阅读顺序**：第一份必读文档

---

### 🔢 专题文档

#### 2. [训练参数计算详解.md](./训练参数计算详解.md) ⭐⭐⭐⭐
**深入解析训练参数计算逻辑（828 行）**

适合人群：需要理解或调整训练参数的用户

内容包括：
- ✅ 核心概念和参数层级关系
- ✅ 参数计算完整流程（6 个阶段）
- ✅ 数据集大小与训练步数的关系
- ✅ 批次大小参数详解（为什么是 524,288？）
- ✅ 不同训练阶段的参数策略
- ✅ 6 个实际计算示例（d4/d10/d16/d20/d26/单GPU）

**核心问题回答**：
- ❓ total_batch_size 为什么是 524,288？
- ❓ 训练步数如何计算？
- ❓ 数据集切片数量如何影响训练？
- ❓ 梯度累积如何工作？

**推荐阅读时机**：准备开始训练或调整参数前

---

#### 3. [模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) ⭐⭐⭐⭐⭐
**不同模型规模详解与中文训练完整方案（900+ 行）**

适合人群：训练中文或中英文模型的用户

内容包括：
- ✅ d4/d6/d8/d10/d12/d16/d20/d26/d32 完整参数对照表
- ✅ 每个规模的详细计算示例
- ✅ 中文语料的特殊性分析
- ✅ 中文/中英文训练的 4 种适配方案
- ✅ 3 个完整实战场景配置
- ✅ 中文分词器训练指南
- ✅ 完整的 speedrun_chinese.sh 脚本

**核心对照表**：

| 模型 | 参数量 | 训练 tokens | 训练时间 | 成本 |
|------|--------|------------|----------|------|
| d4   | 37M    | 0.74B      | 15min    | $6   |
| d8   | 82M    | 1.64B      | 35min    | $14  |
| d10  | 112M   | 2.24B      | 50min    | $20  |
| d16  | 268M   | 5.36B      | 2hr      | $48  |
| d20  | 561M   | 11.2B      | 4hr      | $96  |
| d26  | 1.2B   | 24B        | 12hr     | $288 |
| d32  | 1.9B   | 38B        | 33hr     | $792 |

**中文训练关键差异**：
- 📊 chars_per_token: 英文 4.8 → 中文 1.8-2.5 → 混合 3.0-3.5
- 📈 建议 vocab_size: 英文 65536 → 中文 40000 → 混合 50000
- 🎯 建议 data_ratio: 英文 20× → 中文 25× → 混合 22×

**推荐阅读时机**：
- 需要选择模型规模时
- 准备训练中文或中英文模型时
- 需要优化中文模型性能时

---

## 🚀 快速开始指引

### 场景 1：我想快速了解 nanochat

**推荐路径**：
1. 阅读主项目 [README.md](../README.md)（10 分钟）
2. 浏览 [nanochat技术文档.md](./nanochat技术文档.md) 的「一、项目概述」（15 分钟）
3. 查看「六、总结与最佳实践」中的最佳实践（10 分钟）

**总耗时**：35 分钟

---

### 场景 2：我想训练英文模型（标准流程）

**推荐路径**：
1. 阅读 [nanochat技术文档.md](./nanochat技术文档.md) 的「五、使用指南」→「5.1 快速开始」（20 分钟）
2. 浏览 [训练参数计算详解.md](./训练参数计算详解.md) 的「六、实际计算示例」（15 分钟）
3. 直接运行 `bash speedrun.sh`

**总耗时**：35 分钟阅读 + 4 小时训练

---

### 场景 3：我想训练中文或中英文模型

**推荐路径**：
1. 重点阅读 [模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md)（60 分钟）
2. 根据预算选择模型规模（d8/d16/d20）
3. 按照「四、实战配置建议」中的相应场景操作
4. 参考完整的 speedrun_chinese.sh 脚本

**总耗时**：1 小时阅读 + 准备数据 + 训练时间

---

### 场景 4：我想调整训练参数

**推荐路径**：
1. 阅读 [训练参数计算详解.md](./训练参数计算详解.md) 全文（45 分钟）
2. 理解参数依赖关系和计算公式
3. 使用「七、总结与建议」中的实用建议
4. 小规模实验验证（d4 或 d8）

**总耗时**：45 分钟阅读 + 实验时间

---

### 场景 5：我想理解代码实现

**推荐路径**：
1. 阅读 [nanochat技术文档.md](./nanochat技术文档.md) 的「三、模块功能详解」（60 分钟）
2. 对照文档阅读源码：
   - `nanochat/gpt.py`（模型定义）
   - `nanochat/engine.py`（推理引擎）
   - `scripts/base_train.py`（训练流程）
3. 运行小规模实验（d4）并调试

**总耗时**：2-3 小时

---

## 🎯 按需求查找

### 我想了解...

#### 模型架构
→ [nanochat技术文档.md](./nanochat技术文档.md) § 2.2 模型架构设计

#### 训练流程
→ [nanochat技术文档.md](./nanochat技术文档.md) § 四、训练流程详解

#### 参数计算
→ [训练参数计算详解.md](./训练参数计算详解.md) § 二、参数计算流程

#### 数据处理
→ [nanochat技术文档.md](./nanochat技术文档.md) § 2.4 数据处理流程

#### 不同模型规模
→ [模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 1.2 各规模模型参数对照表

#### 中文训练
→ [模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 三、中文/中英文训练适配策略

#### 命令速查
→ [nanochat技术文档.md](./nanochat技术文档.md) § 附录B 命令速查表

#### 性能优化
→ [nanochat技术文档.md](./nanochat技术文档.md) § 6.2.3 性能优化建议

---

## 📊 文档统计

| 文档 | 行数 | 字数 | 阅读时间 | 难度 |
|------|------|------|----------|------|
| nanochat技术文档 | 1860 | ~60,000 | 3-4 小时 | ⭐⭐⭐ |
| 训练参数计算详解 | 828 | ~25,000 | 1-1.5 小时 | ⭐⭐⭐⭐ |
| 模型规模与中文语料适配指南 | 900+ | ~28,000 | 1.5-2 小时 | ⭐⭐⭐⭐ |
| **总计** | **3600+** | **~113,000** | **5.5-7.5 小时** | - |

---

## ❓ 常见问题快速查找

### Q1: 训练 d20 模型需要多少成本和时间？
**答案**：[模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 1.2 表格
- 成本：~$96（8xH100 @ $24/hr）
- 时间：~4 小时
- 参数：561M
- 训练 tokens：11.2B

---

### Q2: total_batch_size 为什么是 524,288？能改吗？
**答案**：[训练参数计算详解.md](./训练参数计算详解.md) § 4.1
- 这是经验最优值（Chinchilla 论文）
- 可以改变，但需要调整学习率和可能的训练步数
- 详细分析见该章节

---

### Q3: 如何训练中文模型？
**答案**：[模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 三、第 3.2 节
- **必须**：重新训练分词器（中英混合数据）
- **建议**：调整 vocab_size（40k-50k）、增加 data_ratio（25×）
- **可选**：增加 seq_len（3072）
- 完整方案见该章节

---

### Q4: 数据集切片数量不够会怎样？
**答案**：[训练参数计算详解.md](./训练参数计算详解.md) § 3.2
- 数据加载器会**循环读取**（多 epoch）
- 训练仍然完成，但可能过拟合
- 示例计算见 § 6.4

---

### Q5: 单 GPU 能训练吗？
**答案**：[模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 4.3
- 可以！通过梯度累积
- 推荐 d4-d10 模型
- 训练时间 = 8 GPU 时间 × 8
- 完整配置见该章节

---

### Q6: 如何选择模型规模？
**答案**：[模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 1.2 + 五、总结
- 快速实验：d4-d8（$6-$14）
- 学习原型：d10-d16（$20-$48）
- 实用级别：d20-d26（$96-$288）
- 高性能：d32+（$792+）

---

### Q7: 中文的 chars_per_token 是多少？
**答案**：[模型规模与中文语料适配指南.md](./模型规模与中文语料适配指南.md) § 2.1
- 纯英文：4.8
- 纯中文（英文分词器）：1.5-2.0
- 纯中文（中文分词器）：2.5-3.0
- 中英混合：3.0-3.5

---

## 🛠️ 如何贡献

如果你发现文档中的错误或有改进建议，欢迎：
1. 在 GitHub 提 Issue
2. 提交 Pull Request
3. 在 Discussions 中讨论

---

## 📝 文档版本

- **创建日期**：2025 年 11 月 12 日
- **最后更新**：2025 年 11 月 12 日
- **版本**：v1.0.0
- **基于 nanochat**：master 分支（2025 年 11 月）

---

## 📧 联系方式

- **项目地址**：https://github.com/karpathy/nanochat
- **作者**：Andrej Karpathy
- **文档整理**：Community Contributors

---

**祝你训练愉快！🚀**

*Let's train some LLMs!*

